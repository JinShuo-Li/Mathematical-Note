
\chapter{Linear Algebra}
\begin{quote}
\emph{
So long after we finished the first part of the analysis section, we will now move on to the new chapter: Linear Algebra. It's a completely new part of mathematics.
}

\emph{
Unlike analysis and calculus, linear algebra requires more geometric understanding. Here, proof is still important. But what is more crucial for learners is to construct geometric intuition for the subject. We would like to claim that intuition is different from imagination. Though we can only imagine the three dimensional Euclidean space, but our intuition can bring us forward to higher dimension, and more abstract linear spaces.
}

\emph{
In this part, we will start from the topic of solving Systems of Linear Equations, then we will move on to study a special kind of linear space: the Vector Space. Finally, we will move on to a more abstract part: the Linear Space, this equipped us with a new tool to study more general forms of algebraic structure and system.
}

\emph{
Specifically, in the final part focusing on abstract Linear Spaces, we will delve into key concepts such as linear independence, basis, dimension, and linear transformations. Understanding these foundational ideas allows us to unify seemingly disparate mathematical objects—like functions, polynomials, and matrices—under a single, powerful framework. This abstraction is not just an academic exercise; it provides the essential language and tools for tackling complex problems in fields ranging from differential equations and data science to quantum mechanics and engineering optimization. The geometric intuition developed in the study of $\mathbb{R}^n$ will prove indispensable as we navigate these higher-dimensional, abstract realms, cementing linear algebra as one of the most fundamental and broadly applicable areas of modern mathematics.
}

\emph{
In latter chapters, we will introduce another branch of Algebra, the abstract algebra. Unlike Linear algebra that focused more on multivariate equations, abstract algebra study the solution structure of higher-degree equations, which is an even more abstract part of mathematics. But even in abstract algebra we still need the knowledge about linear algebra. So now, let's get started.
}
\end{quote}

\section{Linear Equations and Matrices}

\subsection{Systems of Linear Equations}

I believe most of the readers have seen systems of equations like this:

\[
\begin{cases}
x+y=1\\
x-y =0
\end{cases}
\]

They might have different numbers of variables and equations, but they all shared the same feature: \textbf{Each equation is linear.} Variables are raised only to the first power, with no products between variables (like $xy$), or nonlinear functions (e.g., $\sin(x)$ or $\sqrt{x}$).

For such kind of equation system, we call it the \textbf{system of linear equations}. Their general expression has $m$ equations and $n$ variables (also called an $m \times n$ system):

\[
\begin{cases}
a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n = b_1\\
a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n = b_2\\
\quad \vdots \\
a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n = b_m
\end{cases}
\]
Here, $x_1, \ldots, x_n$ are the \textbf{variables} (unknowns), the $a_{ij}$ are the \textbf{coefficients}, and $b_1, \ldots, b_m$ are the \textbf{constant terms}.

A \textbf{solution} to the system is a set of numbers $(s_1, s_2, \ldots, s_n)$ that satisfies all $m$ equations simultaneously when substituted for $(x_1, x_2, \ldots, x_n)$. The set of all possible solutions is called the \textbf{solution set}.

\begin{example}[A 2x2 System]
Consider the system:
\[
\begin{cases}
x_1 + x_2 = 3 \\
x_1 - x_2 = -1
\end{cases}
\]
We can solve this using simple algebra.
\begin{enumerate}
    \item \textbf{Substitution:} From the first equation, $x_2 = 3 - x_1$. Substitute this into the second: $x_1 - (3 - x_1) = -1$, which gives $2x_1 - 3 = -1$, so $2x_1 = 2$, and $x_1 = 1$. Then $x_2 = 3 - 1 = 2$. The unique solution is $(1, 2)$.
    \item \textbf{Elimination:} Add the two equations: $(x_1 + x_2) + (x_1 - x_2) = 3 + (-1)$, which gives $2x_1 = 2$, so $x_1 = 1$. Subtract the second from the first: $(x_1 + x_2) - (x_1 - x_2) = 3 - (-1)$, which gives $2x_2 = 4$, so $x_2 = 2$.
\end{enumerate}
The solution set is the single point $(1, 2)$.
\end{example}

In linear algebra, we are interested in three questions:
\begin{enumerate}
    \item \textbf{Existence:} Does a solution exist? (Is the system \textbf{consistent}?)
    \item \textbf{Uniqueness:} If a solution exists, is it the only one?
    \item \textbf{Computation:} If solutions exist, how do we find them?
\end{enumerate}

Geometrically, for a 2x2 system, each equation represents a line in the $\mathbb{R}^2$ plane. The solution set is the intersection of these lines.
\begin{itemize}
    \item \textbf{Unique Solution:} The lines intersect at a single point.
    \item \textbf{No Solution:} The lines are parallel and distinct.
    \item \textbf{Infinitely Many Solutions:} The two equations represent the same line.
\end{itemize}
% 

For a 3x3 system, each equation is a plane in $\mathbb{R}^3$. The solution set is the intersection of these three planes, which could be a point, a line, a plane, or empty.
% 

The methods of substitution and elimination become extremely cumbersome for larger systems (e.g., 5 equations, 5 variables). We need a more systematic and efficient approach. This is where matrices come in.

\subsection{Matrix Algebra}

\subsubsection{Matrix Notation and Special Matrices}

The essence of our new method is to manipulate the equations without changing their solution set. We observe that all the information of the system is contained in the coefficients $a_{ij}$ and the constant terms $b_i$. The variable names $x_1, x_2, \ldots$ are just placeholders. We can therefore encode the entire system into a compact rectangular array called a \textbf{matrix}.

\begin{definition}
A \textbf{matrix} is a rectangular array of numbers, called \textbf{entries} or \textbf{elements}. A matrix with $m$ rows and $n$ columns is called an $m \times n$ matrix (read "m by n").
\end{definition}

For the general linear system, we define two key matrices.

The \textbf{coefficient matrix} is:
\[ \textbf{A} = \begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn}
\end{pmatrix} \]
We can denote this matrix as $A = [a_{ij}]$. $a_{ij}$ represents the element in the $i$-th row and $j$-th column.

The \textbf{augmented matrix}, which includes the constant terms, is:
\[ (A \mid \mathbf{b}) = \left(\begin{array}{cccc|c}
a_{11} & a_{12} & \cdots & a_{1n} & b_1 \\
a_{21} & a_{22} & \cdots & a_{2n} & b_2 \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn} & b_m
\end{array}\right) \]
Solving the system is now equivalent to manipulating this augmented matrix.

\begin{example}
The system
\[
\begin{cases}
x_1 - 2x_2 + x_3 = 0 \\
2x_2 - 8x_3 = 8 \\
5x_1 - 5x_3 = 10
\end{cases}
\]
has the coefficient matrix
\[ A = \begin{pmatrix} 1 & -2 & 1 \\ 0 & 2 & -8 \\ 5 & 0 & -5 \end{pmatrix} \]
and the augmented matrix
\[ (A \mid \mathbf{b}) = \left(\begin{array}{ccc|c} 1 & -2 & 1 & 0 \\ 0 & 2 & -8 & 8 \\ 5 & 0 & -5 & 10 \end{array}\right) \]
\end{example}

\paragraph{Matrix Terminology and Special Matrices}

\begin{itemize}
    \item \textbf{Size/Dimension:} A matrix $A$ with $m$ rows and $n$ columns has size $m \times n$.
    \item \textbf{Square Matrix:} A matrix is \textbf{square} if its number of rows equals its number of columns ($m=n$).
    \item \textbf{Equality:} Two matrices $A = [a_{ij}]$ and $B = [b_{ij}]$ are \textbf{equal} if and only if they have the same size ($m \times n$) and all their corresponding entries are equal ($a_{ij} = b_{ij}$ for all $i, j$).
    \item \textbf{Principal Diagonal:} In a square matrix, the entries $a_{11}, a_{22}, \ldots, a_{nn}$ form the \textbf{principal diagonal} (or main diagonal).
    \item \textbf{Auxiliary Diagonal:} In a square matrix, the diagonal from the upper right to the lower left ($a_{1n}, a_{2,n-1}, \ldots, a_{n1}$) is the \textbf{auxiliary diagonal}.
\end{itemize}

There are several special types of matrices:

% 

\begin{enumerate}
    \item \textbf{Null matrix (Zero matrix):} A matrix (of any size) where all entries are 0. It is often denoted $\mathbf{0}$ or $\mathbf{0}_{m \times n}$.
    \[ \mathbf{0} = \begin{pmatrix} 0 & 0 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & 0 \end{pmatrix} \]
    \item \textbf{Identity matrix:} A square matrix $I_n$ (or just $I$) whose entries on the principal diagonal are all 1, and all other entries are 0.
    \[ I_3 = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix} \]
    The identity matrix is the multiplicative identity: $AI = A$ and $IA = A$ (for compatible sizes).
    \item \textbf{Diagonal matrix:} A square matrix where all entries \emph{off} the principal diagonal are 0.
    \[ D = \begin{pmatrix} 3 & 0 & 0 \\ 0 & -1 & 0 \\ 0 & 0 & 5 \end{pmatrix} \]
    \item \textbf{Upper triangular matrix:} A square matrix whose entries \emph{below} the principal diagonal are all 0 ($a_{ij} = 0$ for $i > j$).
    \[ U = \begin{pmatrix} 1 & 4 & -1 \\ 0 & 2 & 7 \\ 0 & 0 & 3 \end{pmatrix} \]
    \item \textbf{Lower triangular matrix:} A square matrix whose entries \emph{above} the principal diagonal are all 0 ($a_{ij} = 0$ for $i < j$).
    \[ L = \begin{pmatrix} 1 & 0 & 0 \\ 5 & 2 & 0 \\ -1 & 0 & 3 \end{pmatrix} \]
    \item \textbf{Transpose:} The \textbf{transpose} of an $m \times n$ matrix $A$, denoted $A^T$ (or $A'$), is the $n \times m$ matrix obtained by interchanging its rows and columns. That is, $(A^T)_{ij} = A_{ji}$.
    \[ A = \begin{pmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{pmatrix} \quad \implies \quad A^T = \begin{pmatrix} 1 & 4 \\ 2 & 5 \\ 3 & 6 \end{pmatrix} \]
    \item \textbf{Symmetric matrix:} A square matrix $A$ such that $A^T = A$. This means $a_{ij} = a_{ji}$ for all $i, j$.
    \[ S = \begin{pmatrix} 1 & 5 & -1 \\ 5 & 2 & 0 \\ -1 & 0 & 3 \end{pmatrix} \]
    \item \textbf{Skew-symmetric matrix:} A square matrix $A$ such that $A^T = -A$. This means $a_{ij} = -a_{ji}$ (and $a_{ii} = 0$).
    \[ K = \begin{pmatrix} 0 & 5 & -1 \\ -5 & 0 & 2 \\ 1 & -2 & 0 \end{pmatrix} \]
\end{enumerate}

\subsubsection{Matrix Operations}

We can define algebraic operations on matrices.

\paragraph{Matrix Addition and Scalar Multiplication}
\begin{definition}
Let $A = [a_{ij}]$ and $B = [b_{ij}]$ be two matrices of the \textbf{same size} $m \times n$.
\begin{enumerate}
    \item \textbf{Addition:} Their sum $A+B$ is the $m \times n$ matrix $C = [c_{ij}]$ where $c_{ij} = a_{ij} + b_{ij}$.
    \item \textbf{Scalar Multiplication:} Let $c$ be a scalar (a real number). The scalar multiple $cA$ is the $m \times n$ matrix $D = [d_{ij}]$ where $d_{ij} = c \cdot a_{ij}$.
\end{enumerate}
Matrix subtraction is defined as $A - B = A + (-1)B$.
\end{definition}

\begin{example}
Let $A = \begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}$ and $B = \begin{pmatrix} 5 & 0 \\ -1 & 7 \end{pmatrix}$.
Then
\[ A+B = \begin{pmatrix} 1+5 & 2+0 \\ 3-1 & 4+7 \end{pmatrix} = \begin{pmatrix} 6 & 2 \\ 2 & 11 \end{pmatrix} \]
\[ 3A = \begin{pmatrix} 3(1) & 3(2) \\ 3(3) & 3(4) \end{pmatrix} = \begin{pmatrix} 3 & 6 \\ 9 & 12 \end{pmatrix} \]
Note that $A + \begin{pmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{pmatrix}$ is \textbf{undefined} as the sizes do not match.
\end{example}

These operations obey familiar properties:
\begin{property}[Properties of Addition and Scalar Multiplication]
Let $A, B, C$ be $m \times n$ matrices and $c, d$ be scalars.
\begin{enumerate}
    \item $A+B = B+A$ (Commutativity of Addition)
    \item $(A+B)+C = A+(B+C)$ (Associativity of Addition)
    \item $A + \mathbf{0} = A$ (Additive Identity)
    \item $A + (-A) = \mathbf{0}$ (Additive Inverse)
    \item $c(A+B) = cA + cB$ (Distributivity)
    \item $(c+d)A = cA + dA$ (Distributivity)
    \item $c(dA) = (cd)A$
    \item $1A = A$
\end{enumerate}
\end{property}
\begin{remark}
These 8 properties, plus closure, are precisely the axioms of a \textbf{Vector Space}. The set $M_{m \times n}$ of all $m \times n$ matrices is a prime example of a vector space.
\end{remark}

\newpage

\paragraph{Matrix Multiplication}
This operation is more complex and profoundly important.

\begin{definition}[Matrix Multiplication]
Let $A$ be an $m \times \mathbf{n}$ matrix and $B$ be an $\mathbf{n} \times p$ matrix. Their \textbf{product} $AB$ is an $m \times p$ matrix $C = [c_{ij}]$.
The entry $c_{ij}$ in the $i$-th row and $j$-th column of $AB$ is computed by taking the \textbf{dot product} of the $i$-th row of $A$ and the $j$-th column of $B$.
\[ c_{ij} = (\text{Row } i \text{ of } A) \cdot (\text{Column } j \text{ of } B) = a_{i1}b_{1j} + a_{i2}b_{2j} + \cdots + a_{in}b_{nj} \]
\[ c_{ij} = \sum_{k=1}^{n} a_{ik}b_{kj} \]
\end{definition}

\textbf{Crucial Note:} The product $AB$ is only defined if the \textbf{number of columns in A} equals the \textbf{number of rows in B}.
\[
(m \times \mathbf{n}) \cdot (\mathbf{n} \times p) \to (m \times p)
\]

\begin{example}
Let $A = \begin{pmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{pmatrix}$ ($2 \times 3$) and $B = \begin{pmatrix} 7 & 8 \\ 9 & 0 \\ 1 & 2 \end{pmatrix}$ ($3 \times 2$).
The product $AB$ will be a $2 \times 2$ matrix.
\[
AB = \begin{pmatrix}
(1\cdot 7 + 2\cdot 9 + 3\cdot 1) & (1\cdot 8 + 2\cdot 0 + 3\cdot 2) \\
(4\cdot 7 + 5\cdot 9 + 6\cdot 1) & (4\cdot 8 + 5\cdot 0 + 6\cdot 2)
\end{pmatrix}
= \begin{pmatrix}
(7 + 18 + 3) & (8 + 0 + 6) \\
(28 + 45 + 6) & (32 + 0 + 12)
\end{pmatrix}
= \begin{pmatrix} 28 & 14 \\ 79 & 44 \end{pmatrix}
\]
Now let's compute $BA$. This will be a $3 \times 3$ matrix.
\[
BA = \begin{pmatrix} 7 & 8 \\ 9 & 0 \\ 1 & 2 \end{pmatrix} \begin{pmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{pmatrix}
= \begin{pmatrix}
(7\cdot 1 + 8\cdot 4) & (7\cdot 2 + 8\cdot 5) & (7\cdot 3 + 8\cdot 6) \\
(9\cdot 1 + 0\cdot 4) & (9\cdot 2 + 0\cdot 5) & (9\cdot 3 + 0\cdot 6) \\
(1\cdot 1 + 2\cdot 4) & (1\cdot 2 + 2\cdot 5) & (1\cdot 3 + 2\cdot 6)
\end{pmatrix}
= \begin{pmatrix} 39 & 54 & 69 \\ 9 & 18 & 27 \\ 9 & 12 & 15 \end{pmatrix}
\]
\end{example}

\begin{property}[Properties of Matrix Multiplication]
Let $A, B, C$ be matrices of compatible sizes and $c$ be a scalar.
\begin{enumerate}
    \item \textbf{Warning:} $AB \neq BA$ in general. Matrix multiplication is \textbf{not commutative}. (See example above).
    \item $A(BC) = (AB)C$ (Associativity)
    \item $A(B+C) = AB + AC$ (Left Distributivity)
    \item $(A+B)C = AC + BC$ (Right Distributivity)
    \item $c(AB) = (cA)B = A(cB)$
    \item $I_m A = A = A I_n$ (Multiplicative Identity)
\end{enumerate}
\end{property}

But we still want the multiplication to have such kind of property like $AB = BA$, we will introduce a special kind of matrix in later chapters that can satisfy this property.

The prooves of the properties above are left for readers.

\begin{remark}
Remember, $\textbf{A} \textbf{b} = \textbf{0}$ can not deduce $A=0$ or $B=0$. This is another counterexample against our common sense of algebraic calculations.
\end{remark}

\begin{definition}
Assume we have a square matrix with $n$ orders, and a number $m \in \mathbb{N}^{*}$, then we call the product of m copies of $A$ "$A$ to the m-th power", denoted as $A^m$
Specifically, we define $A^0 = I_n$.
\end{definition}

The calculations of power is \textbf{BASICALLY} the same with the regular rules. Except those have requirement with the orders of multiplication. Like:

\[(A+B)^2 = A^2 + AB + BA + B^2\]
\[(A+B)(A-B) = A^2 - AB + BA - B^2\]

\begin{definition}
Assume $A=(a_{ij})_{m \times n}$, we define $A^T=(b_{kl})_{n \times m}$ the transpose of matrix $A$, iff $b_{kl} = a_{lk}, (k=1,2,\cdots,n,l=1,2,\cdots,m)$
\end{definition}

\begin{property}[Properties of the Transpose]
\begin{enumerate}
    \item $(A^T)^T = A$
    \item $(A+B)^T = A^T + B^T$
    \item $(cA)^T = cA^T$
    \item \textbf{(Reversal Property)} $(AB)^T = B^T A^T$
    \item $(A^m)^T = (A^T)^m$
\end{enumerate}
\end{property}
\begin{proof}[Proof of $(AB)^T = B^T A^T$]
Let $A$ be $m \times n$ and $B$ be $n \times p$. $AB$ is $m \times p$, so $(AB)^T$ is $p \times m$.
$B^T$ is $p \times n$ and $A^T$ is $n \times m$, so $B^T A^T$ is also $p \times m$. They have the same size.
Let $C = AB$. The $(i, j)$-entry of $C^T$ is $C_{ji}$.
By definition, $C_{ji} = \sum_{k=1}^n A_{jk} B_{ki}$.
Now let $D = B^T A^T$. The $(i, j)$-entry of $D$ is:
\[ D_{ij} = \sum_{k=1}^n (B^T)_{ik} (A^T)_{kj} \]
By definition of transpose, $(B^T)_{ik} = B_{ki}$ and $(A^T)_{kj} = A_{jk}$.
So, $D_{ij} = \sum_{k=1}^n B_{ki} A_{jk} = \sum_{k=1}^n A_{jk} B_{ki}$.
Thus, $D_{ij} = C_{ji} = (C^T)_{ij}$. Since all entries are equal, $B^T A^T = (AB)^T$.
\end{proof}

There is also another important value for square matrix, we will use it in later contents.

\begin{definition}[The trace of matrix]
Assume a square matrix $A$ with $n$ orders $A=(a_{ij})$, $\sum_{i=1}^{n} a_{ii}$ is called the trace of matrix, denoted as $tr(A)$
\end{definition}

\begin{property}
\begin{enumerate}
    \item $tr(A+B) = tr(A) + tr(B)$
    \item $tr(kA) = k tr(A)$
    \item $tr(AB) = tr(BA)$
    \item $tr(A^T) = tr(A)$
\end{enumerate}
\end{property}

\subsubsection*{Definition of Block Matrices}
A block matrix is a matrix that is partitioned into smaller submatrices called \textbf{blocks}. This is done by drawing horizontal and vertical lines that divide the matrix into rectangular blocks. Partitioning allows us to view a large matrix as composed of smaller, more manageable parts.

If \( A \) is an \( m \times n \) matrix, we can partition it as follows:
\[
A = \begin{bmatrix}
    A_{11} & A_{12} & \cdots & A_{1q} \\
    A_{21} & A_{22} & \cdots & A_{2q} \\
    \vdots & \vdots & \ddots & \vdots \\
    A_{p1} & A_{p2} & \cdots & A_{pq}
\end{bmatrix}
\]
Here, each \( A_{ij} \) is a submatrix (block) of \( A \), and the dimensions of the blocks must be consistent: the number of columns in \( A_{ik} \) must equal the number of columns in \( A_{jk} \) for all \( i, j, k \), and similarly for rows.

\textbf{Operations with Block Matrices}

\paragraph*{Block Matrix Addition}
If two matrices \( A \) and \( B \) are partitioned into blocks with the \textbf{same dimensions} for corresponding blocks, they can be added block-wise:
\[
A + B = \begin{bmatrix}
    A_{11} + B_{11} & A_{12} + B_{12} & \cdots \\
    A_{21} + B_{21} & A_{22} + B_{22} & \cdots \\
    \vdots & \vdots & \ddots
\end{bmatrix}
\]
Each block \( A_{ij} \) and \( B_{ij} \) must have the same dimensions for the addition to be valid.

\paragraph*{Block Matrix Scalar Multiplication}
Scalar multiplication is performed by multiplying each block by the scalar:
\[
cA = \begin{bmatrix}
    cA_{11} & cA_{12} & \cdots \\
    cA_{21} & cA_{22} & \cdots \\
    \vdots & \vdots & \ddots
\end{bmatrix}
\]

\paragraph*{Block Matrix Multiplication}
If \( A \) is an \( m \times n \) block matrix and \( B \) is an \( n \times p \) block matrix, and the partitions are such that the number of column blocks of \( A \) equals the number of row blocks of \( B \), then the product \( C = AB \) can be computed block-wise:
\[
C_{ij} = \sum_{k=1}^{q} A_{ik} B_{kj}
\]
This requires that the number of columns in \( A_{ik} \) equals the number of rows in \( B_{kj} \) for each \( k \). The resulting block \( C_{ij} \) is the sum of products of corresponding blocks.

\textbf{Example:} If \( A = \begin{bmatrix} A_{11} & A_{12} \\ A_{21} & A_{22} \end{bmatrix} \) and \( B = \begin{bmatrix} B_{11} \\ B_{21} \end{bmatrix} \), then:
\[
AB = \begin{bmatrix}
    A_{11}B_{11} + A_{12}B_{21} \\
    A_{21}B_{11} + A_{22}B_{21}
\end{bmatrix}
\]

\paragraph*{Block Matrix Transpose}
The transpose of a block matrix is obtained by transposing each block and then transposing the block structure:
\[
A^\top = \begin{bmatrix}
    A_{11}^\top & A_{21}^\top & \cdots \\
    A_{12}^\top & A_{22}^\top & \cdots \\
    \vdots & \vdots & \ddots
\end{bmatrix}
\]
Note that the positions of the blocks are also transposed (e.g., the block in the (1,2) position becomes the block in the (2,1) position after transposition).

\paragraph*{Block Diagonal Matrices}
A block diagonal matrix is a square block matrix where all off-diagonal blocks are zero matrices:
\[
A = \begin{bmatrix}
    A_{11} & 0 & \cdots & 0 \\
    0 & A_{22} & \cdots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \cdots & A_{pp}
\end{bmatrix}
\]
Operations on block diagonal matrices simplify because the blocks can be handled independently (e.g., the inverse of a block diagonal matrix is the block diagonal matrix of the inverses, if they exist).

\subsubsection*{Advantages of Using Block Matrices}
\begin{itemize}
    \item Simplifies operations on large matrices by breaking them into smaller parts.
    \item Facilitates parallel computation.
    \item Helps in proving theoretical results by induction on block structures.
    \item Commonly used in numerical linear algebra for efficient algorithms.
\end{itemize}


\begin{definition}
An $n \times n$ square matrix $A$ is \textbf{invertible} (or \textbf{non-singular}) if there exists an $n \times n$ matrix $B$ such that
\[ AB = I_n \quad \text{and} \quad BA = I_n \]
This matrix $B$ is unique and is called the \textbf{inverse} of $A$, denoted $A^{-1}$.
If no such matrix $B$ exists, $A$ is \textbf{singular} (or \textbf{non-invertible}).
\end{definition}



\begin{example}[Inverse of a 2x2 Matrix]
Let $A = \begin{pmatrix} a & b \\ c & d \end{pmatrix}$.
If $ad-bc \neq 0$, then $A$ is invertible and
\[ A^{-1} = \frac{1}{ad-bc} \begin{pmatrix} d & -b \\ -c & a \end{pmatrix} \]
If $ad-bc = 0$, $A$ is singular. The quantity $ad-bc$ is the \textbf{determinant} of $A$.
\end{example}

\begin{property}[Properties of Inverses]
Let $A$ and $B$ be invertible $n \times n$ matrices.
\begin{enumerate}
    \item $(A^{-1})^{-1} = A$
    \item $(AB)^{-1} = B^{-1} A^{-1}$ (Note the reversal of order)
    \item $(A^T)^{-1} = (A^{-1})^T$
    \item $(cA)^{-1} = \frac{1}{c} A^{-1}$ (for $c \neq 0$)
\end{enumerate}
\end{property}
\begin{proof}[Proof of $(AB)^{-1} = B^{-1} A^{-1}$]
We just need to check the definition.
\[ (AB)(B^{-1} A^{-1}) = A (B B^{-1}) A^{-1} = A (I) A^{-1} = A A^{-1} = I \]
\[ (B^{-1} A^{-1})(AB) = B^{-1} (A^{-1} A) B = B^{-1} (I) B = B^{-1} B = I \]
Since $B^{-1} A^{-1}$ satisfies the definition, it must be the inverse of $AB$.
\end{proof}

\subsection{Solving Systems of Linear Equations}

Now we return to our main problem: solving $m$ equations in $n$ variables.
The general idea, \textbf{Gaussian Elimination}, is to transform the augmented matrix into a simpler form from which we can just read off the solution.

\subsubsection{Elementary Row Operations (EROs)}

We can manipulate the augmented matrix using operations that correspond to manipulating the original equations. These operations \textbf{do not change the solution set}.

\begin{definition}
The three \textbf{elementary row operations (EROs)} are:
\begin{enumerate}
    \item \textbf{(Replacement)} Add to one row a multiple of another row. ($R_i \to R_i + cR_j$)
    \item \textbf{(Interchange)} Interchange two rows. ($R_i \leftrightarrow R_j$)
    \item \textbf{(Scaling)} Multiply all entries in a row by a non-zero constant. ($R_i \to cR_i, c \neq 0$)
\end{enumerate}
\end{definition}

\begin{definition}
Two matrices $A$ and $B$ are \textbf{row equivalent}, denoted $A \sim B$, if $B$ can be obtained from $A$ by a sequence of EROs.
\end{definition}

\begin{theorem}
If the augmented matrices of two linear systems are row equivalent, then the two systems have the \textbf{same solution set}.
\end{theorem}
\begin{proof}[Justification]
\begin{itemize}
    \item (Replacement) $R_i \to R_i + cR_j$ corresponds to adding $c$ times equation $j$ to equation $i$. This is a reversible step (by $R_i \to R_i - cR_j$), and any solution to the original system will also be a solution to the new one, and vice-versa.
    \item (Interchange) $R_i \leftrightarrow R_j$ corresponds to swapping the order of two equations, which clearly does not affect the solution set.
    \item (Scaling) $R_i \to cR_i$ (with $c \neq 0$) corresponds to multiplying an equation by $c$. This is reversible (by $R_i \to \frac{1}{c}R_i$), so it does not change the solution set.
\end{itemize}
\end{proof}

\subsubsection{Row Echelon Form and Rank}

The goal is to use EROs to simplify the matrix into a "staircase" form.

\begin{definition}
A matrix is in \textbf{Row Echelon Form (REF)} if it satisfies:
\begin{enumerate}
    \item All nonzero rows are above any rows of all zeros.
    \item Each \textbf{leading entry} (or \textbf{pivot}), which is the leftmost nonzero entry of a row, is in a column to the right of the leading entry of the row above it.
    \item All entries in a column \emph{below} a leading entry are zeros.
\end{enumerate}
\end{definition}

\begin{definition}
A matrix is in \textbf{Reduced Row Echelon Form (RREF)} if it is in REF and also satisfies:
\begin{enumerate}
    \item The leading entry in each nonzero row is 1.
    \item Each leading 1 is the \emph{only} nonzero entry in its column.
\end{enumerate}
\end{definition}



\begin{example}
\textbf{REF}:
\[ \begin{pmatrix} \fbox{2} & 3 & 4 & 5 \\ 0 & \fbox{1} & 6 & 7 \\ 0 & 0 & 0 & \fbox{8} \\ 0 & 0 & 0 & 0 \end{pmatrix} \]
\textbf{RREF}:
\[ \begin{pmatrix} \fbox{1} & 0 & -1 & 0 \\ 0 & \fbox{1} & 2 & 0 \\ 0 & 0 & 0 & \fbox{1} \\ 0 & 0 & 0 & 0 \end{pmatrix} \]
(Pivots are boxed.)
\end{example}

\begin{theorem}
Every matrix is row equivalent to a \textbf{unique} Reduced Row Echelon Form (RREF).
\end{theorem}

This algorithm to get to REF/RREF is the core of our solution method.

\begin{definition}
\begin{itemize}
    \item \textbf{Gaussian Elimination} is the process of using EROs to transform a matrix into REF.
    \item \textbf{Gauss-Jordan Elimination} is the process of using EROs to transform a matrix into RREF.
\end{itemize}
\end{definition}

\paragraph{The Algorithm (Gauss-Jordan Elimination)}
Let's solve a system completely.
\[
\begin{cases}
x_2 + 3x_3 = 4 \\
x_1 + x_2 + x_3 = 1 \\
2x_1 + 3x_2 + 4x_3 = 7
\end{cases}
\]
The augmented matrix is:
\[
\left(\begin{array}{ccc|c}
0 & 1 & 3 & 4 \\
1 & 1 & 1 & 1 \\
2 & 3 & 4 & 7
\end{array}\right)
\]
\textbf{Step 1: (Forward Phase - Get to REF)}
We need a pivot in the top-left (1,1) position. Swap with R2.
\[
\left(\begin{array}{ccc|c}
1 & 1 & 1 & 1 \\
0 & 1 & 3 & 4 \\
2 & 3 & 4 & 7
\end{array}\right) \quad (R_1 \leftrightarrow R_2)
\]
Create zeros below the first pivot.
\[
\left(\begin{array}{ccc|c}
1 & 1 & 1 & 1 \\
0 & 1 & 3 & 4 \\
0 & 1 & 2 & 5
\end{array}\right) \quad (R_3 \to R_3 - 2R_1)
\]
Now, move to the second pivot (2,2). It's already 1. Create a zero below it.
\[
\left(\begin{array}{ccc|c}
1 & 1 & 1 & 1 \\
0 & 1 & 3 & 4 \\
0 & 0 & -1 & 1
\end{array}\right) \quad (R_3 \to R_3 - R_2)
\]
The matrix is now in \textbf{Row Echelon Form}. This completes Gaussian Elimination.
We could stop here and use \textbf{back substitution}:
From $R_3$: $-x_3 = 1 \implies x_3 = -1$.
From $R_2$: $x_2 + 3x_3 = 4 \implies x_2 + 3(-1) = 4 \implies x_2 = 7$.
From $R_1$: $x_1 + x_2 + x_3 = 1 \implies x_1 + 7 + (-1) = 1 \implies x_1 = -5$.
The unique solution is $(-5, 7, -1)$.

\textbf{Step 2: (Backward Phase - Get to RREF)}
Continue from the REF. Scale all pivots to 1.
\[
\left(\begin{array}{ccc|c}
1 & 1 & 1 & 1 \\
0 & 1 & 3 & 4 \\
0 & 0 & 1 & -1
\end{array}\right) \quad (R_3 \to -1 \cdot R_3)
\]
Create zeros \emph{above} the pivots, starting from the rightmost pivot.
\[
\left(\begin{array}{ccc|c}
1 & 1 & 0 & 2 \\
0 & 1 & 0 & 7 \\
0 & 0 & 1 & -1
\end{array}\right) \quad (R_1 \to R_1 - R_3, R_2 \to R_2 - 3R_3)
\]
Create zero above the second pivot.
\[
\left(\begin{array}{ccc|c}
1 & 0 & 0 & -5 \\
0 & 1 & 0 & 7 \\
0 & 0 & 1 & -1
\end{array}\right) \quad (R_1 \to R_1 - R_2)
\]
This is the \textbf{Reduced Row Echelon Form}. The corresponding system is:
\[
\begin{cases}
x_1 = -5 \\
x_2 = 7 \\
x_3 = -1
\end{cases}
\]
This immediately gives the solution.

\newpage

\paragraph{Rank of a Matrix}
A key concept emerges from the echelon form.

\begin{definition}
The \textbf{rank} of a matrix $A$, denoted $\operatorname{rank}(A)$, is the number of leading entries (pivots) in its row echelon form. This number is unique for any given matrix.
\end{definition}

\begin{property}[Properties of Rank]
Let $A$ be an $m \times n$ matrix.
\begin{enumerate}
    \item $\operatorname{rank}(A) \le \min(m, n)$.
    \item $\operatorname{rank}(A) = 0$ if and only if $A = \mathbf{0}$.
    \item \textbf{(Major Theorem)} $\operatorname{rank}(A) = \operatorname{rank}(A^T)$. (The number of pivot rows equals the number of pivot columns).
    \item $\operatorname{rank}(AB) \le \min(\operatorname{rank}(A), \operatorname{rank}(B))$.
    \item $\operatorname{rank}(A+B) \le \operatorname{rank}(A) + \operatorname{rank}(B)$.
    \item $\operatorname{rank}(A)+\operatorname{rank}(B)-n \le \operatorname{rank}(AB)$
    \item If $P, Q$ are invertible, $\operatorname{rank}(PAQ) = \operatorname{rank}(A)$. EROs are equivalent to multiplying by an invertible matrix on the left, so row operations do not change the rank.
\end{enumerate}
In short, we can denote them as:
\[
\min \{r(A),r(B)\} \geq r(AB) \geq r(A) + r(B) - n
\]
\[
r(A+B) \leq r(A,B) \leq r \begin{pmatrix}
    A & O \\
    O & B
\end{pmatrix}
\]
\end{property}

\subsubsection{Solution Sets of Linear Systems}

The RREF of the \emph{augmented} matrix tells us everything about the solution set.
A \textbf{pivot column} is a column in the coefficient matrix $A$ that contains a pivot in its RREF.
Variables corresponding to pivot columns are called \textbf{basic variables}.
Variables corresponding to non-pivot columns are called \textbf{free variables}.

Let $r = \operatorname{rank}(A)$ for an $m \times n$ coefficient matrix $A$.
We analyze the RREF of the augmented matrix $[A \mid \mathbf{b}]$.

\begin{enumerate}
    \item \textbf{No Solution (Inconsistent System)}
    This occurs if the RREF of $[A \mid \mathbf{b}]$ has a row of the form $\begin{pmatrix} 0 & 0 & \cdots & 0 & \mid & 1 \end{pmatrix}$.
    This corresponds to the impossible equation $0x_1 + \cdots + 0x_n = 1$, or $0 = 1$.
    In terms of rank, this means the last column (the augmented column) is a pivot column.
    \textbf{Condition: $\operatorname{rank}(A) < \operatorname{rank}([A \mid \mathbf{b}])$.}

    \item \textbf{A Solution Exists (Consistent System)}
    This occurs if the augmented column is \emph{not} a pivot column.
    \textbf{Condition: $\operatorname{rank}(A) = \operatorname{rank}([A \mid \mathbf{b}])$.} Let this rank be $r$.
    \begin{itemize}
        \item \textbf{Unique Solution:} The system has a unique solution if there are \emph{no free variables}. This means every variable is a basic variable, so every column of $A$ is a pivot column.
        \textbf{Condition: $r = n$ (the number of variables).}
        
        \item \textbf{Infinitely Many Solutions:} The system has infinitely many solutions if there is \emph{at least one free variable}. This means some columns of $A$ are not pivot columns.
        \textbf{Condition: $r < n$ (the number of variables).}
        The $n - r$ free variables can be set to any arbitrary value (parameters), and the basic variables can be expressed in terms of them.
    \end{itemize}
\end{enumerate}

\paragraph{Parametric Vector Form}
When we have infinitely many solutions, we write the solution set in \textbf{parametric vector form}.

\begin{example}[Infinitely Many Solutions]
Find the general solution to the system with augmented matrix:
\[
\left(\begin{array}{ccc|c}
1 & 0 & -5 & 1 \\
0 & 1 & 1 & 4 \\
0 & 0 & 0 & 0
\end{array}\right)
\]
This matrix is already in RREF. The corresponding system is:
\[
\begin{cases}
x_1 - 5x_3 = 1 \\
x_2 + x_3 = 4 \\
0 = 0
\end{cases}
\]
The pivot columns are 1 and 2. So, $x_1$ and $x_2$ are \textbf{basic variables}.
Column 3 is not a pivot column. So, $x_3$ is a \textbf{free variable}.
We introduce a parameter, $t$, for the free variable. Let $x_3 = t$, where $t$ can be any real number.
Now, we express the basic variables in terms of the free variables:
\[ x_1 = 1 + 5x_3 = 1 + 5t \]
\[ x_2 = 4 - x_3 = 4 - t \]
\[ x_3 = t \]
The general solution $\mathbf{x} = \begin{pmatrix} x_1 \\ x_2 \\ x_3 \end{pmatrix}$ is:
\[ \mathbf{x} = \begin{pmatrix} 1 + 5t \\ 4 - t \\ t \end{pmatrix} = \begin{pmatrix} 1 \\ 4 \\ 0 \end{pmatrix} + t \begin{pmatrix} 5 \\ -1 \\ 1 \end{pmatrix} \]
This is the \textbf{parametric vector form}.
Geometrically, this is the equation of a \textbf{line} in $\mathbb{R}^3$ passing through the point $(1, 4, 0)$ and parallel to the vector $(5, -1, 1)$.
\end{example}

\subsubsection{Homogeneous and Non-homogeneous Systems}

\begin{definition}
A system of linear equations is called \textbf{homogeneous} if it is of the form $A\mathbf{x} = \mathbf{0}$, where $\mathbf{0}$ is the zero vector (all $b_i = 0$).
\[
\begin{cases}
a_{11}x_1 + \cdots + a_{1n}x_n = 0\\
\quad \vdots \\
a_{m1}x_1 + \cdots + a_{mn}x_n = 0
\end{cases}
\]
A system $A\mathbf{x} = \mathbf{b}$ with $\mathbf{b} \neq \mathbf{0}$ is called \textbf{non-homogeneous}.
\end{definition}

A homogeneous system $A\mathbf{x} = \mathbf{0}$ is \textbf{always} consistent, because $\mathbf{x} = \mathbf{0}$ (the zero vector) is always a solution, known as the \textbf{trivial solution}.
The important question is whether a \textbf{non-trivial solution} exists.
This happens if and only if there is at least one free variable, which is equivalent to $\operatorname{rank}(A) < n$ (the number of variables).

\begin{theorem}
The homogeneous system $A\mathbf{x} = \mathbf{0}$ has a non-trivial solution if and only if $\operatorname{rank}(A) < n$.
\end{theorem}
\begin{corollary}
If $A$ is $m \times n$ with $m < n$ (fewer equations than variables, a "wide" matrix), then $A\mathbf{x} = \mathbf{0}$ \textbf{always} has infinitely many solutions, because $\operatorname{rank}(A) \le m < n$.
\end{corollary}

There is a fundamental connection between the solution sets of the two systems.

\begin{theorem}[Structure of Solutions]
Suppose the non-homogeneous system $A\mathbf{x} = \mathbf{b}$ is consistent and has a particular solution $\mathbf{x}_p$. Then the general solution $\mathbf{x}_g$ of $A\mathbf{x} = \mathbf{b}$ is the set of all vectors of the form
\[ \mathbf{x}_g = \mathbf{x}_p + \mathbf{x}_h \]
where $\mathbf{x}_h$ is any solution to the corresponding homogeneous system $A\mathbf{x} = \mathbf{0}$.
\end{theorem}
\begin{proof}
Let $\mathbf{x}_g$ be any solution to $A\mathbf{x} = \mathbf{b}$. Let $\mathbf{x}_h = \mathbf{x}_g - \mathbf{x}_p$.
Then $A\mathbf{x}_h = A(\mathbf{x}_g - \mathbf{x}_p) = A\mathbf{x}_g - A\mathbf{x}_p = \mathbf{b} - \mathbf{b} = \mathbf{0}$.
So, $\mathbf{x}_h$ is a solution to the homogeneous system.
This shows any solution $\mathbf{x}_g$ can be written in the form $\mathbf{x}_p + \mathbf{x}_h$.
Conversely, let $\mathbf{x}_h$ be any homogeneous solution.
Then $A(\mathbf{x}_p + \mathbf{x}_h) = A\mathbf{x}_p + A\mathbf{x}_h = \mathbf{b} + \mathbf{0} = \mathbf{b}$.
So, $\mathbf{x}_p + \mathbf{x}_h$ is a solution to the non-homogeneous system.
\end{proof}

\begin{remark}
Look back at our last example:
\[ \mathbf{x} = \underbrace{\begin{pmatrix} 1 \\ 4 \\ 0 \end{pmatrix}}_{\mathbf{x}_p} + t \underbrace{\begin{pmatrix} 5 \\ -1 \\ 1 \end{pmatrix}}_{\mathbf{x}_h} \]
Here $\mathbf{x}_p = (1, 4, 0)$ is one \emph{particular solution} to $A\mathbf{x} = \mathbf{b}$.
$\mathbf{x}_h = t(5, -1, 1)$ is the \emph{general solution} to the corresponding homogeneous system $A\mathbf{x} = \mathbf{0}$.
Geometrically, the solution set to $A\mathbf{x} = \mathbf{b}$ is a \emph{translation} (by $\mathbf{x}_p$) of the solution set to $A\mathbf{x} = \mathbf{0}$.
% 
\end{remark}

\section{Determinants}

We now study a powerful tool associated with \textbf{square} matrices: the determinant. The determinant is a single number that reveals a wealth of information about a matrix, most notably whether it is invertible.

The calculation of determinants require familiarity and patience, and once we can find other ways to avoid using determinants, we shall do so.

\subsection{The Determinant of a Matrix}

For a $1 \times 1$ matrix $A = (a)$, $\det(A) = a$.
For a $2 \times 2$ matrix, the determinant is simple:
\[ \det(A) = \det \begin{pmatrix} a & b \\ c & d \end{pmatrix} = \begin{vmatrix} a & b \\ c & d \end{vmatrix} = ad - bc \]
This value $ad-bc$ is non-zero if and only if the matrix is invertible.

For larger $n \times n$ matrices, we define the determinant recursively using \textbf{cofactor expansion}.

\begin{definition}
Let $A$ be an $n \times n$ matrix.
\begin{itemize}
    \item The \textbf{minor} $M_{ij}$ of the entry $a_{ij}$ is the determinant of the $(n-1) \times (n-1)$ matrix obtained by deleting the $i$-th row and $j$-th column of $A$.
    \item The \textbf{cofactor} $C_{ij}$ is given by $C_{ij} = (-1)^{i+j} M_{ij}$.
\end{itemize}
\end{definition}

The "checkerboard" pattern of signs for $(-1)^{i+j}$ is $\begin{pmatrix} + & - & + & \cdots \\ - & + & - & \cdots \\ + & - & + & \cdots \\ \vdots & \vdots & \vdots & \ddots \end{pmatrix}$.

\begin{theorem}[Cofactor Expansion]
The determinant of an $n \times n$ matrix $A$ can be found by expanding along \textbf{any} row $i$:
\[ \det(A) = a_{i1}C_{i1} + a_{i2}C_{i2} + \cdots + a_{in}C_{in} = \sum_{j=1}^{n} a_{ij}C_{ij} \]
Alternatively, we can expand down \textbf{any} column $j$:
\[ \det(A) = a_{1j}C_{1j} + a_{2j}C_{2j} + \cdots + a_{nj}C_{nj} = \sum_{i=1}^{n} a_{ij}C_{ij} \]
\end{theorem}

\begin{example}[Cofactor Expansion of 3x3]
Let $A = \begin{pmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{pmatrix}$. Let's expand along Row 1.
\[ \det(A) = 1 \cdot C_{11} + 2 \cdot C_{12} + 3 \cdot C_{13} \]
\[ C_{11} = (-1)^{1+1} \begin{vmatrix} 5 & 6 \\ 8 & 9 \end{vmatrix} = +1(5\cdot 9 - 6\cdot 8) = 45 - 48 = -3 \]
\[ C_{12} = (-1)^{1+2} \begin{vmatrix} 4 & 6 \\ 7 & 9 \end{vmatrix} = -1(4\cdot 9 - 6\cdot 7) = -(36 - 42) = 6 \]
\[ C_{13} = (-1)^{1+3} \begin{vmatrix} 4 & 5 \\ 7 & 8 \end{vmatrix} = +1(4\cdot 8 - 5\cdot 7) = 32 - 35 = -3 \]
\[ \det(A) = 1(-3) + 2(6) + 3(-3) = -3 + 12 - 9 = 0 \]
Since $\det(A) = 0$, this matrix is \textbf{singular} (not invertible).
\end{example}

A more formal definition of determinants relies on the concept of negative sequence. And it is logically equivalent to the definition above so we won't present it again. But here is a more axiomatized definitions about determinant I would like to share with you:

\begin{definition}[The axiomatic definition of determinants]
The determinant is the unique function det: $M_n(\mathbb{R}) \leftarrow \mathbb{R}$ satisfying the following three axioms:
\begin{enumerate}
    \item Multilinearity: It is a linear function of each column when the other columns are held fixed.
    \item Alternating: If two columns of the matrix are identical, then its determinant is zero. This also implies that swapping two columns changes the sign of the determinant.
    \item Normalization: The determinant of the identity matrix is 1.
\end{enumerate}
\end{definition}

Another definition is more \textbf{modern}, it's about exterior product

\begin{definition}[Exterior Product (Wedge Product)]
Let $V$ be a vector space over field $\mathbb{K}$. The \textbf{exterior product} (or \textbf{wedge product}) is a bilinear map:
\[
\wedge: V \times V \to \Lambda^2(V)
\]
satisfying:
\begin{enumerate}
    \item \textbf{Anticommutativity}: $u \wedge v = -v \wedge u$ for all $u, v \in V$
    \item \textbf{Nilpotence}: $v \wedge v = 0$ for all $v \in V$
\end{enumerate}
The $k$-th exterior power $\Lambda^k(V)$ is spanned by elements of the form $v_1 \wedge v_2 \wedge \cdots \wedge v_k$ where $v_i \in V$.
\end{definition}

\begin{definition}[Determinant via Exterior Algebra]
Let $V$ be an $n$-dimensional vector space with basis $\{e_1, \ldots, e_n\}$. A linear operator $T: V \to V$ induces $\Lambda^nT: \Lambda^n(V) \to \Lambda^n(V)$ on the top exterior power:
\[
\Lambda^nT(e_1 \wedge \cdots \wedge e_n) = T(e_1) \wedge \cdots \wedge T(e_n)
\]
Since $\Lambda^n(V)$ is 1-dimensional, there exists a unique scalar $\det(T) \in \mathbb{K}$ such that:
\[
T(e_1) \wedge \cdots \wedge T(e_n) = \det(T) \cdot (e_1 \wedge \cdots \wedge e_n)
\]
This scalar $\det(T)$ is called the \textbf{determinant} of $T$.

For matrix $A = (a_{ij})$ with column vectors $a_1, \ldots, a_n \in \mathbb{R}^n$:
\[
a_1 \wedge a_2 \wedge \cdots \wedge a_n = \det(A) \cdot (e_1 \wedge e_2 \wedge \cdots \wedge e_n)
\]
\end{definition}

\begin{theorem}
The exterior algebra definition implies the axiomatic definition of determinant.
\end{theorem}

\begin{proof}
We verify the three axioms:

1. \textbf{Multilinearity}: The wedge product is linear in each argument:
\[
(\lambda u + \mu v) \wedge w = \lambda(u \wedge w) + \mu(v \wedge w)
\]
Thus $(a_1, \ldots, a_n) \mapsto a_1 \wedge \cdots \wedge a_n$ is multilinear, and so is $\det(A)$.

2. \textbf{Alternating property}: If $a_i = a_j$ ($i \neq j$), then:
\[
a_1 \wedge \cdots \wedge a_i \wedge \cdots \wedge a_j \wedge \cdots \wedge a_n = 0
\]
since $v \wedge v = 0$. Hence $\det(A) = 0$. Swapping columns introduces a sign change due to anticommutativity.

3. \textbf{Normalization}: For identity matrix $I$:
\[
e_1 \wedge \cdots \wedge e_n = \det(I) \cdot (e_1 \wedge \cdots \wedge e_n) \Rightarrow \det(I) = 1
\]
\end{proof}

\begin{corollary}
The multiplicative property $\det(AB) = \det(A)\det(B)$ follows naturally.
\end{corollary}

\begin{proof}
Consider the composition on $\Lambda^n(V)$:
\[
\Lambda^n(AB)(e_1 \wedge \cdots \wedge e_n) = \Lambda^nA(\Lambda^nB(e_1 \wedge \cdots \wedge e_n)) = \det(A)\det(B)(e_1 \wedge \cdots \wedge e_n)
\]
But also equals $\det(AB)(e_1 \wedge \cdots \wedge e_n)$, so $\det(AB) = \det(A)\det(B)$.
\end{proof}

\begin{remark}[Sarrus's Rule for 3x3]
For $3 \times 3$ matrices \emph{only}, there is a shortcut.
% 
Write the first two columns again to the right:
\[
\begin{pmatrix} a & b & c \\ d & e & f \\ g & h & i \end{pmatrix}
\begin{matrix} a & b \\ d & e \\ g & h \end{matrix}
\]
Sum the products of the down-right diagonals and subtract the products of the up-right diagonals:
\[ \det = (aei + bfg + cdh) - (gec + hfa + idb) \]
Using our example:
$(1\cdot 5\cdot 9 + 2\cdot 6\cdot 7 + 3\cdot 4\cdot 8) - (7\cdot 5\cdot 3 + 8\cdot 6\cdot 1 + 9\cdot 4\cdot 2)$
$= (45 + 84 + 96) - (105 + 48 + 72) = 225 - 225 = 0$.
\textbf{Warning:} This \emph{does not} work for 4x4 or larger.
\end{remark}

\subsection{Properties of Determinants}
Calculating determinants via cofactors is computationally slow ($O(n!)$). A more efficient method ($O(n^3)$) uses row operations.

\begin{theorem}[Determinants and EROs]
Let $A$ be an $n \times n$ matrix.
\begin{enumerate}
    \item \textbf{(Replacement)} If $B$ is obtained from $A$ by $R_i \to R_i + cR_j$, then $\det(B) = \det(A)$.
    \item \textbf{(Interchange)} If $B$ is obtained from $A$ by $R_i \leftrightarrow R_j$, then $\det(B) = -\det(A)$.
    \item \textbf{(Scaling)} If $B$ is obtained from $A$ by $R_i \to cR_i$, then $\det(B) = c \cdot \det(A)$.
\end{enumerate}
\end{theorem}

This allows us to row-reduce $A$ to an echelon form $U$ (which is triangular) while keeping track of the changes.
\begin{theorem}
If $A$ is a triangular matrix (upper or lower), its determinant is the product of its diagonal entries.
\[ \det(A) = a_{11} a_{22} \cdots a_{nn} \]
\end{theorem}
\begin{proof}
Expand cofactors along the first row (if lower triangular) or first column (if upper triangular) repeatedly.
\end{proof}

\begin{example}[Calculating $\det$ with EROs]
\[ A = \begin{pmatrix} 0 & 1 & 5 \\ 3 & -6 & 9 \\ 2 & 6 & 1 \end{pmatrix} \]
\[ \det(A) = - \begin{vmatrix} 3 & -6 & 9 \\ 0 & 1 & 5 \\ 2 & 6 & 1 \end{vmatrix} \quad (R_1 \leftrightarrow R_2) \]
\[ \det(A) = -3 \begin{vmatrix} 1 & -2 & 3 \\ 0 & 1 & 5 \\ 2 & 6 & 1 \end{vmatrix} \quad (R_1 \to \frac{1}{3}R_1, \text{pull out } 3) \]
\[ \det(A) = -3 \begin{vmatrix} 1 & -2 & 3 \\ 0 & 1 & 5 \\ 0 & 10 & -5 \end{vmatrix} \quad (R_3 \to R_3 - 2R_1) \]
\[ \det(A) = -3 \begin{vmatrix} 1 & -2 & 3 \\ 0 & 1 & 5 \\ 0 & 0 & -55 \end{vmatrix} \quad (R_3 \to R_3 - 10R_2) \]
The matrix is now triangular.
\[ \det(A) = -3 \cdot (1 \cdot 1 \cdot -55) = 165 \]
\end{example}

\begin{property}[More Properties of Determinants]
Let $A, B$ be $n \times n$ matrices.
\begin{enumerate}
    \item \textbf{(Major Theorem)} $A$ is invertible if and only if $\det(A) \neq 0$.
    \item \textbf{(Multiplicative Property)} $\det(AB) = \det(A)\det(B)$.
    \item $\det(A^T) = \det(A)$. (This implies all ERO properties also work for \emph{columns}).
    \item If $A$ is invertible, $\det(A^{-1}) = \frac{1}{\det(A)}$.
    \item $\det(cA) = c^n \det(A)$ (where $A$ is $n \times n$).
    \item If $A$ has a zero row (or column), $\det(A) = 0$.
    \item If $A$ has two identical rows (or columns), $\det(A) = 0$.
\end{enumerate}
\end{property}
\begin{proof}[Proof of $\det(A^{-1}) = 1/\det(A)$]
$A A^{-1} = I$.
$\det(A A^{-1}) = \det(I)$.
$\det(A) \det(A^{-1}) = 1$.
$\det(A^{-1}) = \frac{1}{\det(A)}$. (This requires $\det(A) \neq 0$, which is true since $A$ is invertible).
\end{proof}

\subsection{Cramer's Rule and Adjoint Formula}

Determinants provide explicit formulas for solving $A\mathbf{x} = \mathbf{b}$ and finding $A^{-1}$. While elegant, they are computationally \emph{inefficient} for large matrices compared to elimination.

\begin{theorem}[Cramer's Rule]
Let $A$ be an invertible $n \times n$ matrix. For any $\mathbf{b}$ in $\mathbb{R}^n$, the unique solution $\mathbf{x}$ of $A\mathbf{x} = \mathbf{b}$ has entries given by
\[ x_i = \frac{\det(A_i(\mathbf{b}))}{\det(A)}, \quad \text{for } i = 1, 2, \ldots, n \]
where $A_i(\mathbf{b})$ is the matrix obtained from $A$ by replacing its $i$-th column with the vector $\mathbf{b}$.
\end{theorem}

\begin{example}
Solve $ \begin{cases} 2x_1 + 5x_2 = -1 \\ 3x_1 + 7x_2 = 4 \end{cases} $
$A = \begin{pmatrix} 2 & 5 \\ 3 & 7 \end{pmatrix}, \mathbf{b} = \begin{pmatrix} -1 \\ 4 \end{pmatrix}$
$\det(A) = 2(7) - 5(3) = 14 - 15 = -1$.
$A_1(\mathbf{b}) = \begin{pmatrix} -1 & 5 \\ 4 & 7 \end{pmatrix}, \det(A_1(\mathbf{b})) = -7 - 20 = -27$.
$A_2(\mathbf{b}) = \begin{pmatrix} 2 & -1 \\ 3 & 4 \end{pmatrix}, \det(A_2(\mathbf{b})) = 8 - (-3) = 11$.
$x_1 = \frac{-27}{-1} = 27$.
$x_2 = \frac{11}{-1} = -11$.
\end{example}

\begin{definition}[Adjoint Matrix]
Let $C = [C_{ij}]$ be the matrix of cofactors of $A$. The \textbf{adjoint} (or \textbf{adjugate}) of $A$, denoted $\operatorname{adj}(A)$, is the \textbf{transpose} of the cofactor matrix.
\[ \operatorname{adj}(A) = C^T \]
\end{definition}

\begin{theorem}[Inverse Formula]
Let $A$ be an invertible matrix. Then
\[ A^{-1} = \frac{1}{\det(A)} \operatorname{adj}(A) \]
\end{theorem}
\begin{remark}
This theorem explains the $2 \times 2$ inverse formula.
For $A = \begin{pmatrix} a & b \\ c & d \end{pmatrix}$:
$C_{11} = d, C_{12} = -c, C_{21} = -b, C_{22} = a$.
Cofactor Matrix $C = \begin{pmatrix} d & -c \\ -b & a \end{pmatrix}$.
Adjoint Matrix $\operatorname{adj}(A) = C^T = \begin{pmatrix} d & -b \\ -c & a \end{pmatrix}$.
$A^{-1} = \frac{1}{ad-bc} \begin{pmatrix} d & -b \\ -c & a \end{pmatrix}$.
\end{remark}

\newpage

\subsection{The Invertible Matrix Theorem (IMT)}
This is one of the most important theorems in linear algebra. It links all the major concepts we have seen so far for a \textbf{square} $n \times n$ matrix $A$.

\begin{theorem}[The Invertible Matrix Theorem]
Let $A$ be a square $n \times n$ matrix. The following statements are equivalent (that is, if any one is true, they are all true, and if any one is false, they are all false).
\begin{enumerate}
    \item $A$ is an invertible matrix.
    \item $A$ is row equivalent to the identity matrix $I_n$.
    \item $A$ has $n$ pivot positions.
    \item The equation $A\mathbf{x} = \mathbf{0}$ has only the trivial solution ($\mathbf{x} = \mathbf{0}$).
    \item The columns of $A$ form a linearly independent set.
    \item The linear transformation $T(\mathbf{x}) = A\mathbf{x}$ is one-to-one.
    \item The equation $A\mathbf{x} = \mathbf{b}$ has at least one solution for \emph{each} $\mathbf{b}$ in $\mathbb{R}^n$.
    \item The columns of $A$ span $\mathbb{R}^n$.
    \item The linear transformation $T(\mathbf{x}) = A\mathbf{x}$ maps $\mathbb{R}^n$ \emph{onto} $\mathbb{R}^n$.
    \item There is an $n \times n$ matrix $C$ such that $CA = I_n$.
    \item There is an $n \times n$ matrix $D$ such that $AD = I_n$.
    \item $A^T$ is an invertible matrix.
    \item $\det(A) \neq 0$.
    \item $\operatorname{rank}(A) = n$.
    \item $\operatorname{Nul}(A) = \{\mathbf{0}\}$ (The null space is the zero vector).
    \item $\operatorname{Col}(A) = \mathbb{R}^n$ (The column space is all of $\mathbb{R}^n$).
    \item 0 is not an eigenvalue of $A$. (We will see this later).
\end{enumerate}
\end{theorem}

This theorem is a powerful diagnostic tool. To check if a square matrix is invertible, we only need to verify \emph{one} of these conditions. For example, checking if $\det(A) \neq 0$ is often the fastest way.

\section{Vectors in $\mathbb{R}^n$}

We now introduce a new and fundamental object: the vector. This allows us to re-interpret systems of equations in a powerful, geometric way.

\subsection{Vectors and Operations}
Geometrically, in two ($\mathbb{R}^2$) or three ($\mathbb{R}^3$) dimensions, we can think of a vector as an arrow with a specific length and direction.


Algebraically, we define a \textbf{vector} in $\mathbb{R}^n$ (read: "R-n") as an ordered $n$-tuple of real numbers. We typically write it as a \textbf{column vector}:
\[ \mathbf{v} = \begin{pmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{pmatrix} \]
The set $\mathbb{R}^n$ is the collection of all such $n$-dimensional vectors. $\mathbb{R}^2$ is the set of all vectors $\begin{pmatrix} x \\ y \end{pmatrix}$, which we identify with the 2D Cartesian plane.
A vector $\mathbf{v} = (v_1, \ldots, v_n)$ can also be written as a \textbf{row vector}, but column vectors are standard when working with matrix equations.

We define two fundamental operations on vectors. Let $\mathbf{u} = \begin{pmatrix} u_1 \\ \vdots \\ u_n \end{pmatrix}$ and $\mathbf{v} = \begin{pmatrix} v_1 \\ \vdots \\ v_n \end{pmatrix}$ be vectors in $\mathbb{R}^n$ and let $c$ be a real number (a \textbf{scalar}).

\begin{enumerate}
    \item \textbf{Vector Addition:} $\mathbf{u} + \mathbf{v}$ is found by adding corresponding components:
    \[ \mathbf{u} + \mathbf{v} = \begin{pmatrix} u_1 + v_1 \\ \vdots \\ u_n + v_n \end{pmatrix} \]
    Geometrically, this corresponds to the \textbf{Parallelogram Law}.
    \item \textbf{Scalar Multiplication:} $c\mathbf{v}$ is found by multiplying each component by $c$:
    \[ c\mathbf{v} = \begin{pmatrix} cv_1 \\ \vdots \\ cv_n \end{pmatrix} \]
    Geometrically, this scales the length of the vector by $|c|$ and reverses its direction if $c < 0$.
\end{enumerate}

These operations satisfy the 8 properties (associativity, commutativity, etc.) listed in Section 2.2.1, making $\mathbb{R}^n$ a prime example of a vector space.

\subsection{Dot Product, Norm, and Orthogonality}

Beyond addition and scaling, we can define a product that gives a scalar in $\mathbb{R}^n$.

\begin{definition}[Dot Product]
The \textbf{dot product} (or \textbf{inner product}) of $\mathbf{u}, \mathbf{v}$ in $\mathbb{R}^n$ is:
\[ \mathbf{u} \cdot \mathbf{v} = u_1v_1 + u_2v_2 + \cdots + u_nv_n = \sum_{i=1}^n u_i v_i \]
Note: $\mathbf{u} \cdot \mathbf{v}$ is a \textbf{scalar}, not a vector.
We can also write this using matrix multiplication: $\mathbf{u} \cdot \mathbf{v} = \mathbf{u}^T \mathbf{v}$.
\end{definition}

\begin{property}[Properties of the Dot Product]
\begin{enumerate}
    \item $\mathbf{u} \cdot \mathbf{v} = \mathbf{v} \cdot \mathbf{u}$ (Commutative)
    \item $(\mathbf{u} + \mathbf{v}) \cdot \mathbf{w} = \mathbf{u} \cdot \mathbf{w} + \mathbf{v} \cdot \mathbf{w}$ (Distributive)
    \item $(c\mathbf{u}) \cdot \mathbf{v} = c(\mathbf{u} \cdot \mathbf{v})$
    \item $\mathbf{u} \cdot \mathbf{u} \ge 0$, and $\mathbf{u} \cdot \mathbf{u} = 0 \iff \mathbf{u} = \mathbf{0}$.
\end{enumerate}
\end{property}

\begin{definition}[Norm and Distance]
\begin{enumerate}
    \item The \textbf{norm} (or \textbf{length}) of a vector $\mathbf{v}$ is:
    \[ \|\mathbf{v}\| = \sqrt{\mathbf{v} \cdot \mathbf{v}} = \sqrt{v_1^2 + v_2^2 + \cdots + v_n^2} \]
    \item A vector $\mathbf{u}$ with $\|\mathbf{u}\| = 1$ is called a \textbf{unit vector}.
    \item \textbf{Normalizing} a vector $\mathbf{v} \neq \mathbf{0}$ means finding the unit vector in its direction: $\mathbf{u} = \frac{1}{\|\mathbf{v}\|} \mathbf{v}$.
    \item The \textbf{distance} between $\mathbf{u}$ and $\mathbf{v}$ is $d(\mathbf{u}, \mathbf{v}) = \|\mathbf{u} - \mathbf{v}\|$.
\end{enumerate}
\end{definition}

However, when dealing with abstract vector spaces, we may not have a natural dot product. In such cases, we can define an \textbf{inner product} that satisfies the same properties as the dot product. We shall see how to do this in later sections.

\begin{definition}[Orthogonality]
Two vectors $\mathbf{u}$ and $\mathbf{v}$ in $\mathbb{R}^n$ are \textbf{orthogonal} (perpendicular) if their dot product is zero:
\[ \mathbf{u} \perp \mathbf{v} \iff \mathbf{u} \cdot \mathbf{v} = 0 \]
The zero vector $\mathbf{0}$ is orthogonal to every vector in $\mathbb{R}^n$.
\end{definition}

Likewise, we can define orthogonality in inner product spaces and weighted dot product spaces by replacing the dot product with the inner product or weighted dot product.

\begin{theorem}[Pythagorean Theorem]
$\|\mathbf{u} + \mathbf{v}\|^2 = \|\mathbf{u}\|^2 + \|\mathbf{v}\|^2$ if and only if $\mathbf{u} \cdot \mathbf{v} = 0$.
\end{theorem}
\begin{proof}
$\|\mathbf{u} + \mathbf{v}\|^2 = (\mathbf{u} + \mathbf{v}) \cdot (\mathbf{u} + \mathbf{v}) = \mathbf{u} \cdot \mathbf{u} + \mathbf{u} \cdot \mathbf{v} + \mathbf{v} \cdot \mathbf{u} + \mathbf{v} \cdot \mathbf{v}$
$\|\mathbf{u} + \mathbf{v}\|^2 = \|\mathbf{u}\|^2 + 2(\mathbf{u} \cdot \mathbf{v}) + \|\mathbf{v}\|^2$.
The equality holds iff $2(\mathbf{u} \cdot \mathbf{v}) = 0$, which means $\mathbf{u} \cdot \mathbf{v} = 0$.
\end{proof}

The dot product also defines the angle between two vectors.

\begin{theorem}
For $\mathbf{u}, \mathbf{v}$ in $\mathbb{R}^n$,
\[ \mathbf{u} \cdot \mathbf{v} = \|\mathbf{u}\| \|\mathbf{v}\| \cos\theta \]
where $\theta$ is the angle between $\mathbf{u}$ and $\mathbf{v}$.
\end{theorem}
This leads to a famous inequality:
\begin{theorem}[Cauchy-Schwarz Inequality]
For all $\mathbf{u}, \mathbf{v}$ in $\mathbb{R}^n$,
\[ |\mathbf{u} \cdot \mathbf{v}| \le \|\mathbf{u}\| \|\mathbf{v}\| \]
\end{theorem}
\begin{theorem}[Triangle Inequality]
For all $\mathbf{u}, \mathbf{v}$ in $\mathbb{R}^n$,
\[ \|\mathbf{u} + \mathbf{v}\| \le \|\mathbf{u}\| + \|\mathbf{v}\| \]
\end{theorem}

\subsection{Linear Combinations and Span}

This is one of the most important ideas in the entire subject.

\begin{definition}
A \textbf{linear combination} of vectors $\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_p$ in $\mathbb{R}^n$ is any vector $\mathbf{y}$ of the form:
\[ \mathbf{y} = c_1\mathbf{v}_1 + c_2\mathbf{v}_2 + \cdots + c_p\mathbf{v}_p \]
where $c_1, \ldots, c_p$ are any scalars (also called weights).
\end{definition}

\begin{example}
In $\mathbb{R}^3$, let $\mathbf{v}_1 = \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}, \mathbf{v}_2 = \begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix}$.
$\mathbf{y} = 3\mathbf{v}_1 + (-2)\mathbf{v}_2 = 3\begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix} - 2\begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix} = \begin{pmatrix} 3 \\ -2 \\ 0 \end{pmatrix}$ is a linear combination of $\mathbf{v}_1, \mathbf{v}_2$.
But $\begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix}$ is \emph{not} a linear combination of $\mathbf{v}_1, \mathbf{v}_2$.
\end{example}

\begin{definition}
The set of \textbf{all possible} linear combinations of $\mathbf{v}_1, \ldots, \mathbf{v}_p$ is called the \textbf{Span} of these vectors, denoted $\operatorname{Span}\{\mathbf{v}_1, \ldots, \mathbf{v}_p\}$.
\end{definition}

Geometrically, the span has a simple interpretation:
\begin{itemize}
    \item $\operatorname{Span}\{\mathbf{v}\}$ (for $\mathbf{v} \neq \mathbf{0}$) is the line through the origin and $\mathbf{v}$.
    \item $\operatorname{Span}\{\mathbf{u}, \mathbf{v}\}$ (for non-collinear $\mathbf{u}, \mathbf{v}$) is the plane containing the origin, $\mathbf{u}$, and $\mathbf{v}$.
    \item $\operatorname{Span}\{\mathbf{0}\}$ is just the set $\{\mathbf{0}\}$, the origin.
\end{itemize}
%

\subsection{The Matrix Equation $A\mathbf{x} = \mathbf{b}$}

We can now connect our topics.
Let $A$ be an $m \times n$ matrix. We can view its columns as $n$ vectors in $\mathbb{R}^m$:
$A = \begin{pmatrix} \mathbf{a}_1 & \mathbf{a}_2 & \cdots & \mathbf{a}_n \end{pmatrix}$.
Let $\mathbf{x} = \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix}$ be a vector in $\mathbb{R}^n$.

\begin{definition}[Matrix-Vector Product]
The product of the $m \times n$ matrix $A$ and the $n \times 1$ vector $\mathbf{x}$, denoted $A\mathbf{x}$, is defined as the \textbf{linear combination of the columns of A using the entries of x as weights}:
\[
A\mathbf{x} = x_1\mathbf{a}_1 + x_2\mathbf{a}_2 + \cdots + x_n\mathbf{a}_n
\]
This product results in an $m \times 1$ vector (a vector in $\mathbb{R}^m$).
\end{definition}

\begin{example}
$\begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix} \begin{pmatrix} 5 \\ -1 \end{pmatrix}
= 5 \begin{pmatrix} 1 \\ 3 \end{pmatrix} - 1 \begin{pmatrix} 2 \\ 4 \end{pmatrix}
= \begin{pmatrix} 5 \\ 15 \end{pmatrix} - \begin{pmatrix} 2 \\ 4 \end{pmatrix}
= \begin{pmatrix} 3 \\ 11 \end{pmatrix}$
Note: This matches the row-column rule for matrix multiplication:
$\begin{pmatrix} 1(5) + 2(-1) \\ 3(5) + 4(-1) \end{pmatrix} = \begin{pmatrix} 3 \\ 11 \end{pmatrix}$.
\end{example}

Now look at our original system of equations:
\[
\begin{cases}
a_{11}x_1 + \cdots + a_{1n}x_n = b_1\\
\quad \vdots \\
a_{m1}x_1 + \cdots + a_{mn}x_n = b_m
\end{cases}
\]
The left side can be written as a vector equation:
\[
x_1 \begin{pmatrix} a_{11} \\ \vdots \\ a_{m1} \end{pmatrix} +
x_2 \begin{pmatrix} a_{12} \\ \vdots \\ a_{m2} \end{pmatrix} + \cdots +
x_n \begin{pmatrix} a_{1n} \\ \vdots \\ a_{mn} \end{pmatrix}
=
\begin{pmatrix} b_1 \\ \vdots \\ b_m
\end{pmatrix}
\]
Using our new definitions, this is precisely:
\[ x_1\mathbf{a}_1 + x_2\mathbf{a}_2 + \cdots + x_n\mathbf{a}_n = \mathbf{b} \]
Which is identical to the \textbf{matrix equation}:
\[ A\mathbf{x} = \mathbf{b} \]

This gives us three equivalent ways to view the same problem:
\begin{enumerate}
    \item A system of $m$ linear equations in $n$ variables.
    \item A vector equation $x_1\mathbf{a}_1 + \cdots + x_n\mathbf{a}_n = \mathbf{b}$.
    \item A matrix equation $A\mathbf{x} = \mathbf{b}$.
\end{enumerate}

This is a profound re-interpretation! The question "Does the system $A\mathbf{x} = \mathbf{b}$ have a solution?" is identical to the question:

\begin{center}
\textbf{"Is the vector $\mathbf{b}$ a linear combination of the column vectors of $A$?"}
\end{center}
Or, more simply: \textbf{"Is $\mathbf{b}$ in $\operatorname{Span}\{\mathbf{a}_1, \ldots, \mathbf{a}_n\}$?"}

\begin{theorem}
The equation $A\mathbf{x} = \mathbf{b}$ has a solution if and only if $\mathbf{b}$ is in the span of the columns of $A$. This span is called the \textbf{Column Space} of $A$, denoted $\operatorname{Col}(A)$.
\end{theorem}

\subsection{Linear Independence}
We now ask a related question. What if $\mathbf{b} = \mathbf{0}$?
The equation $A\mathbf{x} = \mathbf{0}$ (or $x_1\mathbf{a}_1 + \cdots + x_n\mathbf{a}_n = \mathbf{0}$) is the \textbf{homogeneous equation}.
We know this *always* has the \textbf{trivial solution} $\mathbf{x} = \mathbf{0}$ (i.e., $x_1=0, \ldots, x_n=0$).
But does it have \emph{only} the trivial solution?

\begin{definition}
A set of vectors $\{\mathbf{v}_1, \ldots, \mathbf{v}_p\}$ in $\mathbb{R}^n$ is said to be \textbf{linearly independent} if the vector equation
\[ c_1\mathbf{v}_1 + c_2\mathbf{v}_2 + \cdots + c_p\mathbf{v}_p = \mathbf{0} \]
has \textbf{only} the trivial solution ($c_1 = c_2 = \cdots = c_p = 0$).

The set is \textbf{linearly dependent} if there exist weights $c_i$, \emph{not all zero}, such that the equation holds. This is called a \textbf{linear dependence relation}.
\end{definition}

\begin{example}
Check if $\{\mathbf{v}_1, \mathbf{v}_2, \mathbf{v}_3\} = \left\{ \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}, \begin{pmatrix} 0 \\ 1 \\ 1 \end{pmatrix}, \begin{pmatrix} 2 \\ 3 \\ 3 \end{pmatrix} \right\}$ is linearly independent.
We must solve $c_1\mathbf{v}_1 + c_2\mathbf{v}_2 + c_3\mathbf{v}_3 = \mathbf{0}$.
This is the matrix equation $A\mathbf{c} = \mathbf{0}$ where $A = \begin{pmatrix} \mathbf{v}_1 & \mathbf{v}_2 & \mathbf{v}_3 \end{pmatrix}$.
\[ \left(\begin{array}{ccc|c} 1 & 0 & 2 & 0 \\ 0 & 1 & 3 & 0 \\ 0 & 1 & 3 & 0 \end{array}\right) \sim \left(\begin{array}{ccc|c} 1 & 0 & 2 & 0 \\ 0 & 1 & 3 & 0 \\ 0 & 0 & 0 & 0 \end{array}\right) \quad (R_3 \to R_3 - R_2) \]
$c_3$ is a free variable! So there are non-trivial solutions.
Let $c_3 = t$. Then $c_2 = -3t$ and $c_1 = -2t$.
For $t=1$, we get $c_1 = -2, c_2 = -3, c_3 = 1$.
This gives the linear dependence relation:
\[ -2\mathbf{v}_1 - 3\mathbf{v}_2 + 1\mathbf{v}_3 = \mathbf{0} \quad \text{or} \quad \mathbf{v}_3 = 2\mathbf{v}_1 + 3\mathbf{v}_2 \]
The set is \textbf{linearly dependent}.
\end{example}

\begin{property}[Secondary Conclusions on Independence]
\begin{itemize}
    \item A set of two vectors $\{\mathbf{v}_1, \mathbf{v}_2\}$ is linearly dependent if and only if one is a scalar multiple of the other.
    \item A set is linearly dependent if and only if at least one vector in the set is a linear combination of the others.
    \item Any set containing the zero vector ($\{\mathbf{v}_1, \ldots, \mathbf{0}, \ldots, \mathbf{v}_p\}$) is linearly dependent.
    \item \textbf{(Key Theorem)} If a set contains \emph{more vectors than entries} in each vector (e.g., $p$ vectors in $\mathbb{R}^n$ where $p > n$), the set is \textbf{linearly dependent}.
\end{itemize}
\end{property}
\begin{proof}[Proof of $p > n$ implies dependent]
Let the set be $\{\mathbf{v}_1, \ldots, \mathbf{v}_p\}$ in $\mathbb{R}^n$.
Form the $n \times p$ matrix $A = \begin{pmatrix} \mathbf{v}_1 & \cdots & \mathbf{v}_p \end{pmatrix}$.
We want to solve $A\mathbf{x} = \mathbf{0}$.
This is a homogeneous system with $n$ equations and $p$ variables.
Since $p > n$ (more variables than equations), there must be at least $p - n > 0$ free variables.
The existence of free variables guarantees a non-trivial solution.
Therefore, the columns are linearly dependent.
\end{proof}

Connecting this to matrices, we see that:
\begin{itemize}
    \item The columns of a matrix $A$ are linearly independent if and only if the homogeneous system $A\mathbf{x} = \mathbf{0}$ has only the trivial solution.
    \item This happens if and only if there are no free variables, i.e., $\operatorname{rank}(A) = n$ (every column is a pivot column).
\end{itemize}
This forms several more lines of the Invertible Matrix Theorem.

\section{Linear Transformations}
The matrix-vector product $A\mathbf{x}$ can be viewed as an \emph{action} or \emph{function}. The matrix $A$ \emph{transforms} the vector $\mathbf{x}$ into a new vector $A\mathbf{x}$.

\subsection{Matrix Transformations}
A \textbf{transformation} (or function, or mapping) $T$ from $\mathbb{R}^n$ to $\mathbb{R}^m$ is a rule that assigns to each vector $\mathbf{x}$ in $\mathbb{R}^n$ a vector $T(\mathbf{x})$ in $\mathbb{R}^m$.
\[ T: \mathbb{R}^n \to \mathbb{R}^m \]
\begin{itemize}
    \item $\mathbb{R}^n$ is the \textbf{domain} of $T$.
    \item $\mathbb{R}^m$ is the \textbf{codomain} of $T$.
    \item $T(\mathbf{x})$ is the \textbf{image} of $\mathbf{x}$ under $T$.
    \item The set of all images $T(\mathbf{x})$ is the \textbf{range} of $T$.
\end{itemize}

An important class of transformations are matrix transformations.
For an $m \times n$ matrix $A$, the transformation $T(\mathbf{x}) = A\mathbf{x}$ maps $\mathbb{R}^n \to \mathbb{R}^m$.

\begin{example}
Let $A = \begin{pmatrix} 1 & -3 \\ 3 & 5 \\ -1 & 7 \end{pmatrix}$. This $A$ defines $T: \mathbb{R}^2 \to \mathbb{R}^3$.
Let $\mathbf{x} = \begin{pmatrix} 2 \\ -1 \end{pmatrix}$.
$T(\mathbf{x}) = A\mathbf{x} = \begin{pmatrix} 1 & -3 \\ 3 & 5 \\ -1 & 7 \end{pmatrix} \begin{pmatrix} 2 \\ -1 \end{pmatrix} = \begin{pmatrix} 1(2) - 3(-1) \\ 3(2) + 5(-1) \\ -1(2) + 7(-1) \end{pmatrix} = \begin{pmatrix} 5 \\ 1 \\ -9 \end{pmatrix}$.
The image of $\begin{pmatrix} 2 \\ -1 \end{pmatrix}$ is $\begin{pmatrix} 5 \\ 1 \\ -9 \end{pmatrix}$.
\end{example}

\subsection{Linearity}
Matrix transformations $T(\mathbf{x}) = A\mathbf{x}$ have special properties that come from the properties of matrix multiplication:
\begin{enumerate}
    \item $A(\mathbf{u} + \mathbf{v}) = A\mathbf{u} + A\mathbf{v}$
    \item $A(c\mathbf{u}) = c(A\mathbf{u})$
\end{enumerate}

\begin{definition}
A transformation $T: \mathbb{R}^n \to \mathbb{R}^m$ is \textbf{linear} if for all $\mathbf{u}, \mathbf{v}$ in $\mathbb{R}^n$ and all scalars $c$:
\begin{enumerate}
    \item $T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) + T(\mathbf{v})$ (Preserves addition)
    \item $T(c\mathbf{u}) = cT(\mathbf{u})$ (Preserves scalar multiplication)
\end{enumerate}
\end{definition}
These two rules imply $T(\mathbf{0}) = \mathbf{0}$ and the "superposition principle":
$T(c_1\mathbf{v}_1 + \cdots + c_p\mathbf{v}_p) = c_1T(\mathbf{v}_1) + \cdots + c_pT(\mathbf{v}_p)$.

\begin{theorem}
Every matrix transformation $T(\mathbf{x}) = A\mathbf{x}$ is a linear transformation.
\end{theorem}
The more powerful fact is that the reverse is also true.

\newpage

\subsection{The Standard Matrix}
\begin{theorem}
Let $T: \mathbb{R}^n \to \mathbb{R}^m$ be a linear transformation. Then there exists a \textbf{unique} $m \times n$ matrix $A$ such that
\[ T(\mathbf{x}) = A\mathbf{x} \quad \text{for all } \mathbf{x} \in \mathbb{R}^n \]
This matrix $A$ is called the \textbf{standard matrix} for $T$ and is given by:
\[ A = \begin{pmatrix} T(\mathbf{e}_1) & T(\mathbf{e}_2) & \cdots & T(\mathbf{e}_n) \end{pmatrix} \]
where $\mathbf{e}_j = \begin{pmatrix} 0 \\ \vdots \\ 1 \\ \vdots \\ 0 \end{pmatrix}$ (1 in $j$-th position) is the $j$-th standard basis vector for $\mathbb{R}^n$.
\end{theorem}
\begin{proof}
Any vector $\mathbf{x} \in \mathbb{R}^n$ can be written as $\mathbf{x} = x_1\mathbf{e}_1 + \cdots + x_n\mathbf{e}_n$.
Since $T$ is linear:
\[ T(\mathbf{x}) = T(x_1\mathbf{e}_1 + \cdots + x_n\mathbf{e}_n) = x_1T(\mathbf{e}_1) + \cdots + x_nT(\mathbf{e}_n) \]
This is a linear combination of the vectors $T(\mathbf{e}_j)$.
By the definition of $A\mathbf{x}$, this is exactly:
\[ T(\mathbf{x}) = \begin{pmatrix} T(\mathbf{e}_1) & T(\mathbf{e}_2) & \cdots & T(\mathbf{e}_n) \end{pmatrix} \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix} = A\mathbf{x} \]
\end{proof}

\subsection{Geometric Transformations in $\mathbb{R}^2$}
This section allows us to find the matrix for geometric operations.

\begin{example}[Rotation]
Find the standard matrix for $T: \mathbb{R}^2 \to \mathbb{R}^2$ that rotates a vector counter-clockwise by an angle $\theta$.
We just need to find $T(\mathbf{e}_1)$ and $T(\mathbf{e}_2)$.
$\mathbf{e}_1 = \begin{pmatrix} 1 \\ 0 \end{pmatrix}$. Rotating this by $\theta$ gives $T(\mathbf{e}_1) = \begin{pmatrix} \cos\theta \\ \sin\theta \end{pmatrix}$.
$\mathbf{e}_2 = \begin{pmatrix} 0 \\ 1 \end{pmatrix}$. Rotating this by $\theta$ gives $T(\mathbf{e}_2) = \begin{pmatrix} -\sin\theta \\ \cos\theta \end{pmatrix}$.
The standard matrix is $A = \begin{pmatrix} T(\mathbf{e}_1) & T(\mathbf{e}_2) \end{pmatrix} = \begin{pmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{pmatrix}$.
\end{example}

\begin{example}[Reflection]
Find the standard matrix for $T: \mathbb{R}^2 \to \mathbb{R}^2$ that reflects a vector across the $x$-axis.
$T(\mathbf{e}_1) = T\begin{pmatrix} 1 \\ 0 \end{pmatrix} = \begin{pmatrix} 1 \\ 0 \end{pmatrix}$.
$T(\mathbf{e}_2) = T\begin{pmatrix} 0 \\ 1 \end{pmatrix} = \begin{pmatrix} 0 \\ -1 \end{pmatrix}$.
The standard matrix is $A = \begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix}$.
\end{example}

\begin{definition}[Kernel and Range]
Let $T: \mathbb{R}^n \to \mathbb{R}^m$ be a linear transformation.
\begin{itemize}
    \item The \textbf{Kernel} of $T$, $\operatorname{Ker}(T)$, is the set of all $\mathbf{x}$ in $\mathbb{R}^n$ such that $T(\mathbf{x}) = \mathbf{0}$.
    \item The \textbf{Range} of $T$, $\operatorname{Range}(T)$, is the set of all $\mathbf{y}$ in $\mathbb{R}^m$ such that $\mathbf{y} = T(\mathbf{x})$ for some $\mathbf{x}$ in $\mathbb{R}^n$.
\end{itemize}
$T$ is \textbf{one-to-one} if $\operatorname{Ker}(T) = \{\mathbf{0}\}$.
$T$ is \textbf{onto} if $\operatorname{Range}(T) = \mathbb{R}^m$.
\end{definition}
If $T(\mathbf{x}) = A\mathbf{x}$, these are just our old subspaces:
\begin{itemize}
    \item $\operatorname{Ker}(T)$ is the solution set of $A\mathbf{x} = \mathbf{0}$. This is the \textbf{Null Space} of $A$, $\operatorname{Nul}(A)$.
    \item $\operatorname{Range}(T)$ is the set of all linear combinations of the columns of $A$. This is the \textbf{Column Space} of $A$, $\operatorname{Col}(A)$.
\end{itemize}

\section{Abstract Linear Spaces and Subspaces}
In the previous sections, we studied $\mathbb{R}^n$ and its algebraic properties. We observed that matrices ($M_{m \times n}$) and polynomials ($\mathcal{P}_n$) also have similar properties (we can add them, scale them). We will now \textbf{abstract} these properties to define a more general concept.

\subsection{The Formal Definition}

\begin{definition}
A \textbf{Linear Space} (or \textbf{Vector Space}) $V$ is a non-empty set of objects, called \textbf{vectors}, on which two operations are defined: vector addition ($\mathbf{u} + \mathbf{v}$) and scalar multiplication ($c\mathbf{u}$) (over a field $F$, usually $\mathbb{R}$). These operations must satisfy the following ten axioms for all vectors $\mathbf{u}, \mathbf{v}, \mathbf{w}$ in $V$ and all scalars $c, d$ in $\mathbb{R}$:

\begin{enumerate}
    \item $\mathbf{u} + \mathbf{v}$ is in $V$. (Closure under addition)
    \item $\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}$. (Commutativity)
    \item $(\mathbf{u} + \mathbf{v}) + \mathbf{w} = \mathbf{u} + (\mathbf{v} + \mathbf{w})$. (Associativity of addition)
    \item There is a \textbf{zero vector} $\mathbf{0}$ in $V$ such that $\mathbf{u} + \mathbf{0} = \mathbf{u}$.
    \item For each $\mathbf{u}$ in $V$, there is an \textbf{additive inverse} $-\mathbf{u}$ in $V$ such that $\mathbf{u} + (-\mathbf{u}) = \mathbf{0}$.
    \item $c\mathbf{u}$ is in $V$. (Closure under scalar multiplication)
    \item $c(\mathbf{u} + \mathbf{v}) = c\mathbf{u} + c\mathbf{v}$. (Distributivity)
    \item $(c+d)\mathbf{u} = c\mathbf{u} + d\mathbf{u}$. (Distributivity)
    \item $c(d\mathbf{u}) = (cd)\mathbf{u}$. (Associativity of multiplication)
    \item $1\mathbf{u} = \mathbf{u}$. (Scalar identity element)
\end{enumerate}
\end{definition}

\subsection{Examples of Linear Spaces}
The power of this definition comes from the variety of sets that satisfy these axioms.
\begin{itemize}
    \item \textbf{Example 1: $\mathbb{R}^n$}
    As we've just seen, $\mathbb{R}^n$ with standard component-wise operations is our prototype vector space.
    
    \item \textbf{Example 2: The Space of Polynomials $\mathcal{P}_n$}
    Let $V = \mathcal{P}_n$ be the set of all polynomials of degree \textbf{at most} $n$. A "vector" in this space is a polynomial $\mathbf{p}(t) = a_0 + a_1t + \cdots + a_nt^n$.
    Standard polynomial addition and scalar multiplication satisfy all ten axioms. The "zero vector" is the zero polynomial, $\mathbf{0}(t) = 0$.
    
    \item \textbf{Example 3: The Space of Matrices $M_{m \times n}$}
    The set $V = M_{m \times n}$ of all $m \times n$ matrices, with standard matrix addition and scalar multiplication (as defined in Section 2.2), forms a vector space. The "zero vector" is the $m \times n$ zero matrix.
    
    \item \textbf{Example 4: The Space of Functions $C[a, b]$}
    Let $V = C[a, b]$ be the set of all \emph{continuous} real-valued functions on an interval $[a, b]$.
    We define operations "pointwise":
    $(f+g)(x) = f(x) + g(x)$
    $(cf)(x) = c \cdot f(x)$
    Since the sum of continuous functions is continuous, and a scalar multiple is continuous, the set is closed. The "zero vector" is the constant function $f(x) = 0$. This forms a vector space.
\end{itemize}

\subsection{Subspaces}
Often, a vector space is contained inside a larger one.

\begin{definition}
A \textbf{subspace} of a vector space $V$ is a subset $H$ of $V$ that satisfies three properties:
\begin{enumerate}
    \item The zero vector of $V$ is in $H$. ($\mathbf{0} \in H$)
    \item $H$ is closed under vector addition: For all $\mathbf{u}, \mathbf{v}$ in $H$, $\mathbf{u} + \mathbf{v}$ is in $H$.
    \item $H$ is closed under scalar multiplication: For all $\mathbf{u}$ in $H$ and scalar $c$, $c\mathbf{u}$ is in $H$.
\end{enumerate}
These three properties guarantee that $H$ is itself a vector space (it inherits the other 7 axioms from $V$).
\end{definition}

\begin{example}[A subspace]
Let $V = \mathbb{R}^3$. Let $H$ be the $xy$-plane, i.e., $H = \left\{ \begin{pmatrix} x \\ y \\ 0 \end{pmatrix} \mid x, y \in \mathbb{R} \right\}$.
Is $H$ a subspace?
\begin{enumerate}
    \item Is $\mathbf{0} \in H$? Yes, $\begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix}$ has $z=0$.
    \item Let $\mathbf{u} = \begin{pmatrix} u_1 \\ u_2 \\ 0 \end{pmatrix}, \mathbf{v} = \begin{pmatrix} v_1 \\ v_2 \\ 0 \end{pmatrix}$ be in $H$.
    Is $\mathbf{u} + \mathbf{v} \in H$? $\mathbf{u} + \mathbf{v} = \begin{pmatrix} u_1+v_1 \\ u_2+v_2 \\ 0 \end{pmatrix}$. Yes, its third component is 0.
    \item Let $\mathbf{u} = \begin{pmatrix} u_1 \\ u_2 \\ 0 \end{pmatrix}$ be in $H$. Is $c\mathbf{u} \in H$?
    $c\mathbf{u} = \begin{pmatrix} cu_1 \\ cu_2 \\ c \cdot 0 \end{pmatrix} = \begin{pmatrix} cu_1 \\ cu_2 \\ 0 \end{pmatrix}$. Yes, it is in $H$.
\end{enumerate}
Thus, $H$ is a subspace of $\mathbb{R}^3$.
\end{example}

\begin{example}[A non-subspace]
Let $V = \mathbb{R}^2$. Let $H$ be the first quadrant, $H = \left\{ \begin{pmatrix} x \\ y \end{pmatrix} \mid x \ge 0, y \ge 0 \right\}$.
\begin{enumerate}
    \item $\mathbf{0} \in H$. (Pass)
    \item $H$ is closed under addition. (Pass: $x_1+x_2 \ge 0, y_1+y_2 \ge 0$)
    \item Is $H$ closed under scalar multiplication? Let $c = -1$ and $\mathbf{u} = \begin{pmatrix} 1 \\ 1 \end{pmatrix} \in H$.
    $c\mathbf{u} = -1 \begin{pmatrix} 1 \\ 1 \end{pmatrix} = \begin{pmatrix} -1 \\ -1 \end{pmatrix}$. This is \emph{not} in $H$.
\end{enumerate}
$H$ is \textbf{not} a subspace.
\end{example}

\begin{theorem}
If $\mathbf{v}_1, \ldots, \mathbf{v}_p$ are in a vector space $V$, then $H = \operatorname{Span}\{\mathbf{v}_1, \ldots, \mathbf{v}_p\}$ is \textbf{always} a subspace of $V$.
\end{theorem}
\begin{proof}
\begin{enumerate}
    \item $\mathbf{0} = 0\mathbf{v}_1 + \cdots + 0\mathbf{v}_p$, so $\mathbf{0}$ is in the span.
    \item Let $\mathbf{u} = c_1\mathbf{v}_1 + \cdots + c_p\mathbf{v}_p$ and $\mathbf{v} = d_1\mathbf{v}_1 + \cdots + d_p\mathbf{v}_p$.
    Then $\mathbf{u} + \mathbf{v} = (c_1+d_1)\mathbf{v}_1 + \cdots + (c_p+d_p)\mathbf{v}_p$, which is a linear combination, so it is in the span.
    \item Let $k$ be a scalar. $k\mathbf{u} = k(c_1\mathbf{v}_1 + \cdots + c_p\mathbf{v}_p) = (kc_1)\mathbf{v}_1 + \cdots + (kc_p)\mathbf{v}_p$, which is also in the span.
\end{enumerate}
Thus, any span is a subspace.
\end{proof}

\subsection{Null Spaces and Column Spaces}
There are two fundamental subspaces associated with any $m \times n$ matrix $A$.

\begin{definition}
\begin{itemize}
    \item The \textbf{Null Space} of $A$, $\operatorname{Nul}(A)$, is the set of all solutions to the homogeneous equation $A\mathbf{x} = \mathbf{0}$.
    \[ \operatorname{Nul}(A) = \{ \mathbf{x} \in \mathbb{R}^n \mid A\mathbf{x} = \mathbf{0} \} \]
    This is a subspace of $\mathbb{R}^n$.
    
    \item The \textbf{Column Space} of $A$, $\operatorname{Col}(A)$, is the span of the columns of $A$.
    \[ \operatorname{Col}(A) = \operatorname{Span}\{\mathbf{a}_1, \ldots, \mathbf{a}_n\} = \{ \mathbf{b} \in \mathbb{R}^m \mid \mathbf{b} = A\mathbf{x} \text{ for some } \mathbf{x} \in \mathbb{R}^n \} \]
    This is a subspace of $\mathbb{R}^m$.
\end{itemize}
\end{definition}

$\operatorname{Nul}(A)$ describes the structure of the homogeneous solution set.
$\operatorname{Col}(A)$ describes the set of all $\mathbf{b}$ for which $A\mathbf{x} = \mathbf{b}$ is consistent.

\subsection{Basis and Dimension}
We now unify the ideas of spanning and linear independence.

\begin{definition}
A \textbf{basis} for a vector space $V$ is a set of vectors $\mathcal{B} = \{\mathbf{b}_1, \ldots, \mathbf{b}_p\}$ in $V$ such that:
\begin{enumerate}
    \item $\mathcal{B}$ is a linearly independent set.
    \item $\mathcal{B}$ spans $V$ (i.e., $\operatorname{Span}\{\mathcal{B}\} = V$).
\end{enumerate}
\end{definition}
A basis is the "smallest" possible spanning set and the "largest" possible linearly independent set.

\begin{example}
The set of standard vectors $\mathcal{E} = \{\mathbf{e}_1, \ldots, \mathbf{e}_n\}$ is the \textbf{standard basis} for $\mathbb{R}^n$.
The set $\{1, t, t^2, \ldots, t^n\}$ is the \textbf{standard basis} for $\mathcal{P}_n$.
\end{example}

\begin{theorem}
All bases for a vector space $V$ have the same number of vectors.
\end{theorem}

\begin{definition}
The \textbf{dimension} of a non-zero vector space $V$, denoted $\dim(V)$, is the number of vectors in any basis for $V$. The dimension of the zero subspace $\{\mathbf{0}\}$ is defined to be 0.
\end{definition}

\newpage

\textbf{Examples of Dimension:}
\begin{itemize}
    \item $\dim(\mathbb{R}^n) = n$.
    \item $\dim(\mathcal{P}_n) = n+1$ (because of the $t^0=1$ term).
    \item $\dim(M_{m \times n}) = m \times n$.
    \item $C[a, b]$ is \textbf{infinite-dimensional}.
\end{itemize}

There is an interesting conclusion. The cardinality of $C[0,1]$ equals to the cardinality of $\mathbb{R}$.

\begin{proof}
Let $D = \mathbb{Q} \cap [0,1]$ be the countable dense set of rationals in $[0,1]$.

\textbf{Upper bound ($\#C[0,1] \leq \mathfrak{c}$):} Define $\Phi: C[0,1] \to \mathbb{R}^\mathbb{N}$ by
\[
\Phi(f) = (f(q_1), f(q_2), f(q_3), \dots)
\]
where $\{q_i\}$ enumerates $D$. If $\Phi(f) = \Phi(g)$, then $f(q) = g(q)$ for all $q \in D$. By continuity and density, $f = g$ on $[0,1]$, so $\Phi$ is injective. Thus
\[
\#C[0,1] \leq \#(\mathbb{R}^\mathbb{N}) = \mathfrak{c}^{\aleph_0} = (2^{\aleph_0})^{\aleph_0} = 2^{\aleph_0} = \mathfrak{c}.
\]

\textbf{Lower bound ($\#C[0,1] \geq \mathfrak{c}$):} The constant functions $\{f_r(x) = r : r \in \mathbb{R}\}$ form a subset of $C[0,1]$ with cardinality $\mathfrak{c}$.

By Cantor-Bernstein theorem, $\#C[0,1] = \mathfrak{c}$.
\end{proof}

We can now find bases for our two favorite subspaces.
\begin{itemize}
    \item \textbf{Basis for $\operatorname{Col}(A)$:}
    The pivot columns of the \emph{original} matrix $A$ form a basis for $\operatorname{Col}(A)$.
    (Do not use the RREF columns, as EROs change the column space).
    
    \item \textbf{Basis for $\operatorname{Nul}(A)$:}
    The vectors found when writing the solution of $A\mathbf{x} = \mathbf{0}$ in parametric vector form form a basis for $\operatorname{Nul}(A)$.
\end{itemize}

\begin{example}
$A = \begin{pmatrix} 1 & 0 & 2 \\ 0 & 1 & 3 \\ 0 & 1 & 3 \end{pmatrix} \sim \begin{pmatrix} 1 & 0 & 2 \\ 0 & 1 & 3 \\ 0 & 0 & 0 \end{pmatrix}$
\textbf{Column Space:} Pivots are in columns 1 and 2.
Basis for $\operatorname{Col}(A) = \left\{ \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}, \begin{pmatrix} 0 \\ 1 \\ 1 \end{pmatrix} \right\}$.
$\dim(\operatorname{Col}(A)) = 2$.

\textbf{Null Space:} Solve $A\mathbf{x} = \mathbf{0}$.
$x_1 + 2x_3 = 0 \implies x_1 = -2x_3$
$x_2 + 3x_3 = 0 \implies x_2 = -3x_3$
$x_3$ is free. Let $x_3 = t$.
$\mathbf{x} = \begin{pmatrix} -2t \\ -3t \\ t \end{pmatrix} = t \begin{pmatrix} -2 \\ -3 \\ 1 \end{pmatrix}$.
Basis for $\operatorname{Nul}(A) = \left\{ \begin{pmatrix} -2 \\ -3 \\ 1 \end{pmatrix} \right\}$.
$\dim(\operatorname{Nul}(A)) = 1$.
\end{example}

Notice the connection to rank:
\begin{itemize}
    \item $\dim(\operatorname{Col}(A))$ = (Number of pivot columns) = $\operatorname{rank}(A)$.
    \item $\dim(\operatorname{Nul}(A))$ = (Number of free variables) = $n - \operatorname{rank}(A)$.
\end{itemize}
This leads to one of the most important theorems in linear algebra.

\begin{theorem}[The Rank-Nullity Theorem]
For an $m \times n$ matrix $A$,
\[ \dim(\operatorname{Col}(A)) + \dim(\operatorname{Nul}(A)) = n \]
or, equivalently,
\[ \operatorname{rank}(A) + \operatorname{nullity}(A) = n \]
where $n$ is the number of \textbf{columns} and $\operatorname{nullity}(A) = \dim(\operatorname{Nul}(A))$.
\end{theorem}
In our last example, $n=3$. $\operatorname{rank}(A) = 2$, $\operatorname{nullity}(A) = 1$.
$2 + 1 = 3$. The theorem holds.
This theorem beautifully ties together the dimensions of the two fundamental subspaces associated with a matrix.

\subsection{Coordinate Systems}
A basis $\mathcal{B} = \{\mathbf{b}_1, \ldots, \mathbf{b}_n\}$ for $\mathbb{R}^n$ acts like a new coordinate system.
Because $\mathcal{B}$ spans $\mathbb{R}^n$ and is linearly independent, every $\mathbf{x} \in \mathbb{R}^n$ can be written \emph{uniquely} as
\[ \mathbf{x} = c_1\mathbf{b}_1 + \cdots + c_n\mathbf{b}_n \]
\begin{definition}
The scalars $c_1, \ldots, c_n$ are the \textbf{coordinates of $\mathbf{x}$ relative to the basis $\mathcal{B}$}.
The \textbf{coordinate vector} of $\mathbf{x}$ (relative to $\mathcal{B}$) is
\[ [\mathbf{x}]_{\mathcal{B}} = \begin{pmatrix} c_1 \\ \vdots \\ c_n \end{pmatrix} \]
\end{definition}

Let $P_{\mathcal{B}}$ be the \textbf{change-of-coordinates matrix} $P_{\mathcal{B}} = \begin{pmatrix} \mathbf{b}_1 & \cdots & \mathbf{b}_n \end{pmatrix}$.
The equation $\mathbf{x} = c_1\mathbf{b}_1 + \cdots + c_n\mathbf{b}_n$ is just the matrix equation
\[ \mathbf{x} = P_{\mathcal{B}} [\mathbf{x}]_{\mathcal{B}} \]
Since the columns of $P_{\mathcal{B}}$ are a basis, $P_{\mathcal{B}}$ is invertible (by the IMT).
\[ [\mathbf{x}]_{\mathcal{B}} = P_{\mathcal{B}}^{-1} \mathbf{x} \]
This provides a way to "translate" between the standard coordinate system $\mathcal{E}$ and the new system $\mathcal{B}$.

\subsection{Eigenvalues and Eigenvectors}

Eigenvalues and eigenvectors are fundamental concepts in linear algebra that provide crucial insights into the structure of linear transformations. They play a central role in many applications, including vibration analysis, quantum mechanics, and data analysis.

The reason why we wants to study eigenvalues and eigenvectors is that they help us understand how a linear transformation (represented by a matrix) acts on certain special directions in space. Specifically, an eigenvector is a direction that remains unchanged (up to scaling) when the transformation is applied, and the corresponding eigenvalue indicates how much the vector is stretched or compressed.

\begin{definition}[Eigenvalues and Eigenvectors]
Let $A$ be an $n \times n$ square matrix. A scalar $\lambda$ is called an \textbf{eigenvalue} of $A$ if there exists a nonzero vector $\mathbf{v} \in \mathbb{R}^n$ such that
\[
A\mathbf{v} = \lambda \mathbf{v}
\]
The vector $\mathbf{v}$ is called an \textbf{eigenvector} corresponding to the eigenvalue $\lambda$.
\end{definition}

Geometrically, an eigenvector $\mathbf{v}$ is a vector whose direction remains unchanged when transformed by $A$; it is only scaled by the factor $\lambda$.

To find eigenvalues, we rewrite the equation $A\mathbf{v} = \lambda \mathbf{v}$ as
\[
(A - \lambda I)\mathbf{v} = \mathbf{0}
\]
This is a homogeneous system of linear equations. Since $\mathbf{v} \neq \mathbf{0}$, this system must have nontrivial solutions, which requires that the matrix $A - \lambda I$ be singular, i.e., its determinant must be zero.

\begin{definition}[Characteristic Polynomial]
The \textbf{characteristic polynomial} of a matrix $A$ is defined as
\[
p(\lambda) = \det(A - \lambda I)
\]
This is an $n$th-degree polynomial in $\lambda$. The eigenvalues of $A$ are the roots of the characteristic equation $p(\lambda) = 0$.
\end{definition}

The roots of the characteristic polynomial may be real or complex. Repeated roots are called eigenvalues with algebraic multiplicity greater than 1. Each eigenvalue corresponds to an eigenspace.

\begin{definition}[Eigenspace]
For an eigenvalue $\lambda$, the corresponding \textbf{eigenspace} is the solution space of the homogeneous system $(A - \lambda I)\mathbf{v} = \mathbf{0}$, i.e., $\operatorname{Nul}(A - \lambda I)$. The dimension of the eigenspace is called the \textbf{geometric multiplicity} of $\lambda$.
\end{definition}

\begin{example}
Find the eigenvalues and eigenvectors of $A = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}$.

The characteristic polynomial is:
\[
p(\lambda) = \det \begin{pmatrix} 2-\lambda & 1 \\ 1 & 2-\lambda \end{pmatrix} = (2-\lambda)^2 - 1 = \lambda^2 - 4\lambda + 3 = (\lambda-1)(\lambda-3)
\]
The eigenvalues are $\lambda_1 = 1$ and $\lambda_2 = 3$.

For $\lambda_1 = 1$, solve $(A - I)\mathbf{v} = \mathbf{0}$:
\[
\begin{pmatrix} 1 & 1 \\ 1 & 1 \end{pmatrix} \begin{pmatrix} v_1 \\ v_2 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix} \implies v_1 + v_2 = 0
\]
Thus the eigenvectors are $\mathbf{v}_1 = t \begin{pmatrix} 1 \\ -1 \end{pmatrix}$, $t \neq 0$.

For $\lambda_2 = 3$, solve $(A - 3I)\mathbf{v} = \mathbf{0}$:
\[
\begin{pmatrix} -1 & 1 \\ 1 & -1 \end{pmatrix} \begin{pmatrix} v_1 \\ v_2 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix} \implies -v_1 + v_2 = 0
\]
Thus the eigenvectors are $\mathbf{v}_2 = s \begin{pmatrix} 1 \\ 1 \end{pmatrix}$, $s \neq 0$.
\end{example}

\subsection{Diagonalization}

Diagonalization is the process of transforming a matrix into diagonal form, which greatly simplifies computations involving matrix powers and exponentials. The geometric interpretation is that diagonalization aligns the coordinate system with the eigenvectors of the matrix, making the transformation represented by the matrix easier to understand.

\begin{definition}[Diagonalizable Matrix]
An $n \times n$ matrix $A$ is said to be \textbf{diagonalizable} if there exists an invertible matrix $P$ and a diagonal matrix $D$ such that
\[
A = PDP^{-1}
\]
Equivalently, $P^{-1}AP = D$.
\end{definition}

The diagonal entries of $D$ are the eigenvalues of $A$, and the columns of $P$ are the corresponding linearly independent eigenvectors.

\begin{theorem}
A matrix $A$ is diagonalizable if and only if it has $n$ linearly independent eigenvectors. This is equivalent to the condition that the geometric multiplicity of each eigenvalue equals its algebraic multiplicity (the multiplicity as a root of the characteristic polynomial).
\end{theorem}

\begin{example}
Continuing the previous example, $A = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}$ has eigenvalues $\lambda_1=1$ and $\lambda_2=3$ with corresponding eigenvectors $\mathbf{v}_1 = \begin{pmatrix} 1 \\ -1 \end{pmatrix}$ and $\mathbf{v}_2 = \begin{pmatrix} 1 \\ 1 \end{pmatrix}$. These eigenvectors are linearly independent, so we can take
\[
P = \begin{pmatrix} 1 & 1 \\ -1 & 1 \end{pmatrix}, \quad D = \begin{pmatrix} 1 & 0 \\ 0 & 3 \end{pmatrix}
\]
It's easy to verify that $A = PDP^{-1}$.
\end{example}

Once diagonalized, computing powers of $A$ becomes straightforward:
\[
A^k = (PDP^{-1})^k = PD^k P^{-1}
\]
since $D^k$ is simply obtained by raising each diagonal element to the $k$th power.

\subsection{Inner Product Spaces}

An inner product generalizes the dot product and provides a framework for defining lengths, angles, and orthogonality in vector spaces.

\begin{definition}[Inner Product]
Let $V$ be a real vector space. An \textbf{inner product} is a function $\langle \cdot, \cdot \rangle: V \times V \to \mathbb{R}$ satisfying the following properties for all $\mathbf{u}, \mathbf{v}, \mathbf{w} \in V$ and all scalars $c$:
\begin{enumerate}
    \item $\langle \mathbf{u}, \mathbf{v} \rangle = \langle \mathbf{v}, \mathbf{u} \rangle$ (Symmetry)
    \item $\langle \mathbf{u} + \mathbf{v}, \mathbf{w} \rangle = \langle \mathbf{u}, \mathbf{w} \rangle + \langle \mathbf{v}, \mathbf{w} \rangle$ (Linearity)
    \item $\langle c\mathbf{u}, \mathbf{v} \rangle = c\langle \mathbf{u}, \mathbf{v} \rangle$
    \item $\langle \mathbf{u}, \mathbf{u} \rangle \ge 0$, and $\langle \mathbf{u}, \mathbf{u} \rangle = 0$ if and only if $\mathbf{u} = \mathbf{0}$ (Positive definiteness)
\end{enumerate}
\end{definition}

The most common example is the dot product (standard inner product) on $\mathbb{R}^n$:
\[
\langle \mathbf{u}, \mathbf{v} \rangle = \mathbf{u} \cdot \mathbf{v} = u_1 v_1 + u_2 v_2 + \cdots + u_n v_n
\]

\begin{definition}[Norm and Distance]
The \textbf{norm} (length) induced by an inner product is defined as
\[
\|\mathbf{v}\| = \sqrt{\langle \mathbf{v}, \mathbf{v} \rangle}
\]
The \textbf{distance} between vectors $\mathbf{u}$ and $\mathbf{v}$ is $d(\mathbf{u}, \mathbf{v}) = \|\mathbf{u} - \mathbf{v}\|$.
\end{definition}

\begin{definition}[Orthogonality]
Two vectors $\mathbf{u}$ and $\mathbf{v}$ are \textbf{orthogonal} if $\langle \mathbf{u}, \mathbf{v} \rangle = 0$. A set of vectors is orthogonal if all pairs of distinct vectors in the set are orthogonal. If, in addition, each vector has unit norm, the set is \textbf{orthonormal}.
\end{definition}

\subsection{Orthogonal Bases and the Gram-Schmidt Process}

In inner product spaces, orthogonal bases simplify many computations.

\begin{theorem}
If $\{\mathbf{v}_1, \ldots, \mathbf{v}_k\}$ is an orthogonal basis for a subspace $H$, then for any $\mathbf{y} \in H$,
\[
\mathbf{y} = c_1 \mathbf{v}_1 + \cdots + c_k \mathbf{v}_k, \quad \text{where } c_i = \frac{\langle \mathbf{y}, \mathbf{v}_i \rangle}{\langle \mathbf{v}_i, \mathbf{v}_i \rangle}
\]
If the basis is orthonormal, then $c_i = \langle \mathbf{y}, \mathbf{v}_i \rangle$.
\end{theorem}

The Gram-Schmidt process converts any linearly independent set into an orthogonal basis.

\begin{theorem}[Gram-Schmidt Orthogonalization Process]
Let $\{\mathbf{x}_1, \ldots, \mathbf{x}_p\}$ be a basis for a subspace $H$. Define:
\begin{align*}
\mathbf{v}_1 &= \mathbf{x}_1 \\
\mathbf{v}_2 &= \mathbf{x}_2 - \frac{\langle \mathbf{x}_2, \mathbf{v}_1 \rangle}{\langle \mathbf{v}_1, \mathbf{v}_1 \rangle} \mathbf{v}_1 \\
\mathbf{v}_3 &= \mathbf{x}_3 - \frac{\langle \mathbf{x}_3, \mathbf{v}_1 \rangle}{\langle \mathbf{v}_1, \mathbf{v}_1 \rangle} \mathbf{v}_1 - \frac{\langle \mathbf{x}_3, \mathbf{v}_2 \rangle}{\langle \mathbf{v}_2, \mathbf{v}_2 \rangle} \mathbf{v}_2 \\
&\vdots \\
\mathbf{v}_p &= \mathbf{x}_p - \sum_{i=1}^{p-1} \frac{\langle \mathbf{x}_p, \mathbf{v}_i \rangle}{\langle \mathbf{v}_i, \mathbf{v}_i \rangle} \mathbf{v}_i
\end{align*}
Then $\{\mathbf{v}_1, \ldots, \mathbf{v}_p\}$ is an orthogonal basis for $H$. Normalizing each vector yields an orthonormal basis.
\end{theorem}

\subsection{Symmetric Matrices and Quadratic Forms}

Real symmetric matrices have particularly nice properties that make them important in many applications.

\begin{theorem}[Spectral Theorem]
Let $A$ be an $n \times n$ real symmetric matrix. Then:
\begin{enumerate}
    \item All eigenvalues of $A$ are real.
    \item $A$ has $n$ linearly independent eigenvectors, and eigenvectors corresponding to distinct eigenvalues are orthogonal.
    \item $A$ is orthogonally diagonalizable: there exists an orthogonal matrix $Q$ (satisfying $Q^T = Q^{-1}$) and a diagonal matrix $D$ such that
    \[
    A = QDQ^T
    \]
\end{enumerate}
\end{theorem}

Quadratic forms are homogeneous polynomials of degree 2 that can be represented in matrix form as $\mathbf{x}^T A \mathbf{x}$, where $A$ is a symmetric matrix.

\begin{definition}[Quadratic Form]
A \textbf{quadratic form} is a function $Q: \mathbb{R}^n \to \mathbb{R}$ defined by
\[
Q(\mathbf{x}) = \mathbf{x}^T A \mathbf{x} = \sum_{i=1}^n \sum_{j=1}^n a_{ij} x_i x_j
\]
where $A$ is a symmetric matrix.
\end{definition}

Through the orthogonal transformation $\mathbf{x} = Q\mathbf{y}$, a quadratic form can be reduced to its canonical form:
\[
Q(\mathbf{x}) = \mathbf{y}^T D \mathbf{y} = \lambda_1 y_1^2 + \lambda_2 y_2^2 + \cdots + \lambda_n y_n^2
\]
where the $\lambda_i$ are the eigenvalues of $A$.

Quadratic forms are classified based on the signs of their eigenvalues:
\begin{itemize}
    \item \textbf{Positive definite}: All eigenvalues positive; $Q(\mathbf{x}) > 0$ for $\mathbf{x} \neq \mathbf{0}$.
    \item \textbf{Negative definite}: All eigenvalues negative.
    \item \textbf{Indefinite}: Eigenvalues have mixed signs.
\end{itemize}

This classification has important applications in optimization and the study of critical points in multivariable calculus.

\newpage

\subsection{Singular Value Decomposition (SVD)}
The Diagonalization Theorem ($A = PDP^{-1}$) applies only to square, diagonalizable matrices. The Spectral Theorem applies only to symmetric matrices. The SVD is the ultimate generalization: it applies to \textbf{any} $m \times n$ matrix.

\begin{theorem}[Singular Value Decomposition]
Let $A$ be an $m \times n$ matrix with rank $r$. Then there exists an $m \times n$ factorization of the form:
\[ A = U \Sigma V^T \]
where:
\begin{itemize}
    \item $U$ is an $m \times m$ orthogonal matrix ($U^T U = I$). The columns of $U$ are called the \textbf{left singular vectors}.
    \item $V$ is an $n \times n$ orthogonal matrix ($V^T V = I$). The columns of $V$ are called the \textbf{right singular vectors}.
    \item $\Sigma$ is an $m \times n$ rectangular diagonal matrix with non-negative entries on the diagonal:
    \[ \Sigma = \begin{pmatrix} D & 0 \\ 0 & 0 \end{pmatrix} \]
    where $D = \text{diag}(\sigma_1, \sigma_2, \dots, \sigma_r)$ and $\sigma_1 \ge \sigma_2 \ge \dots \ge \sigma_r > 0$.
\end{itemize}
The scalars $\sigma_i$ are called the \textbf{singular values} of $A$. They are the square roots of the non-zero eigenvalues of $A^T A$.
\end{theorem}

\textbf{Geometric Interpretation:}
Any linear transformation $T(\mathbf{x}) = A\mathbf{x}$ maps the unit sphere in the domain to a hyperellipse in the codomain. The singular values are the lengths of the semi-axes of this hyperellipse.

\textbf{Construction of SVD:}
\begin{enumerate}
    \item Compute the eigenvalues of the symmetric matrix $A^T A$. Let them be $\lambda_1 \ge \dots \ge \lambda_n$.
    \item The singular values are $\sigma_i = \sqrt{\lambda_i}$.
    \item Find orthonormal eigenvectors of $A^T A$; these form the matrix $V$.
    \item The first $r$ columns of $U$ are given by $\mathbf{u}_i = \frac{1}{\sigma_i} A \mathbf{v}_i$. Extend this set to an orthonormal basis for $\mathbb{R}^m$ to fill the rest of $U$.
\end{enumerate}

\section{Conclusions}

In this chapter, we have learned a lot of concepts. From matrix to determinant, from rank to eigenvalue $\cdots$. We can spot very beautiful symmetry between each part, we are actually using one single language to describe different things from the same perspective but have varying results. This is what makes mathematics attractive. Now we will focus on two concepts about Linear Algebra, to conclude what we have learned through out the journey:

\subsection{Interpretations of Rank}

Let $A$ be an $m \times n$ matrix over a field $\mathbb{F}$ (e.g., $\mathbb{R}$ or $\mathbb{C}$). The rank of $A$, denoted as $\text{rank}(A)$ or $\rho(A)$, can be defined and interpreted in the following equivalent ways:

\subsubsection{Vector Space Interpretations}
\begin{itemize}
    \item \textbf{Column Rank:} The dimension of the column space of $A$ (the vector space spanned by its columns).
    \[ \text{rank}(A) = \dim(\text{Col}(A)) \]
    \item \textbf{Row Rank:} The dimension of the row space of $A$ (the vector space spanned by its rows). A fundamental property is that row rank equals column rank:
    \[ \dim(\text{Row}(A)) = \dim(\text{Col}(A)) \]
    \item \textbf{Linear Independence:} The maximum number of linearly independent column vectors (or row vectors) in the matrix.
\end{itemize}

\subsubsection{Computational/Algebraic Interpretations}
\begin{itemize}
    \item \textbf{Pivot Definition:} The number of pivots (leading 1s) in the Reduced Row Echelon Form (RREF) of $A$.
    \item \textbf{Determinantal Rank:} The order of the largest non-zero square minor of $A$. That is, $r$ is the rank if there exists an $r \times r$ submatrix with a non-zero determinant, and every $(r+1) \times (r+1)$ minor is zero.
    \item \textbf{Decomposition Rank:} The smallest integer $k$ such that $A$ can be factored as $A = CR$, where $C$ is $m \times k$ and $R$ is $k \times n$.
\end{itemize}

\subsubsection{Geometric and Mapping Interpretations}
\begin{itemize}
    \item \textbf{Image Dimension:} If we view $A$ as a linear transformation $T: \mathbb{F}^n \to \mathbb{F}^m$ defined by $T(\mathbf{x}) = A\mathbf{x}$, the rank is the dimension of the image (range) of $T$:
    \[ \text{rank}(A) = \dim(\text{Im}(T)) \]
    \item \textbf{Singular Value Decomposition (SVD):} The number of non-zero singular values of $A$.
\end{itemize}

\subsection{The Rank-Nullity Theorem}

The Rank-Nullity Theorem (often called the Fundamental Theorem of Linear Algebra) relates the dimensions of the domain, the image, and the kernel. Below are its expressions in different contexts.

\subsubsection{1. Matrix Context}
For an $m \times n$ matrix $A$:
\begin{theorem}[Matrix Rank-Nullity]
The number of columns equals the sum of the rank and the nullity.
\[ \text{rank}(A) + \text{nullity}(A) = n \]
\end{theorem}
\begin{itemize}
    \item \textbf{rank($A$):} The number of pivot columns (basic variables).
    \item \textbf{nullity($A$):} The dimension of the null space ($\dim(\text{Null}(A))$), which corresponds to the number of free columns (free variables).
    \item \textbf{Interpretation:} Total Variables = Pivot Variables + Free Variables.
\end{itemize}

\subsubsection{2. Linear Transformation Context}
Let $V$ and $W$ be vector spaces, where $V$ is finite-dimensional. Let $T: V \to W$ be a linear transformation.
\begin{theorem}[Linear Map Rank-Nullity]
\[ \dim(\text{Im}(T)) + \dim(\ker(T)) = \dim(V) \]
\end{theorem}
\begin{itemize}
    \item $\dim(\text{Im}(T))$ is the rank of the transformation.
    \item $\dim(\ker(T))$ is the nullity (dimension of the kernel).
    \item Note that the sum equals the dimension of the \textit{domain}, not the codomain.
\end{itemize}

\subsubsection{3. Abstract Algebra Context (Isomorphism Theorems)}
The theorem is a direct consequence of the \textbf{First Isomorphism Theorem} for vector spaces (or modules).
\begin{theorem}
\[ V / \ker(T) \cong \text{Im}(T) \]
\end{theorem}
Taking dimensions of both sides:
\[ \dim(V) - \dim(\ker(T)) = \dim(\text{Im}(T)) \]
Rearranging this yields the standard Rank-Nullity equation.

\subsubsection{4. Systems of Linear Equations}
Consider the homogeneous system $A\mathbf{x} = \mathbf{0}$, where $A$ is $m \times n$.
\begin{itemize}
    \item The dimension of the solution space is $k = n - r$, where $r = \text{rank}(A)$.
    \item If $r = n$ (full column rank), the only solution is the trivial solution ($\mathbf{0}$), so nullity is 0.
    \item If $r < m$ (for the augmented system $A\mathbf{x}=\mathbf{b}$), existence of solutions depends on column space consistency.
\end{itemize}

\subsection{The Axiom of Linear Algebra}
Afterall, we need to answer the question: what is the core axiom of the linear algebra? We believe it is \textbf{Axiomatic Definition of a Vector Space}.

The axiomatic definition of vector space is central to linear algebra because it captures the essence of linearity through just two fundamental operations—addition and scalar multiplication—and the eight axioms that govern them. This simple yet powerful abstract framework unifies countless mathematical objects, from geometric vectors to functions and matrices, and provides the common foundation for all core theories, such as linear transformations and solving linear systems. In this way, it serves as the universal language that bridges mathematical theory and scientific application.

We shall present the definition again here.

Let $V$ be a nonempty set whose elements are called \textbf{vectors}, and let $\mathbb{F}$ be a \textbf{field} (such as the real numbers $\mathbb{R}$ or the complex numbers $\mathbb{C}$). Two operations are defined on $V$:

\begin{itemize}
    \item \textbf{Vector addition}: $+ : V \times V \to V$, denoted by $(\mathbf{u}, \mathbf{v}) \mapsto \mathbf{u} + \mathbf{v}$
    \item \textbf{Scalar multiplication}: $\cdot : \mathbb{F} \times V \to V$, denoted by $(c, \mathbf{v}) \mapsto c \mathbf{v}$
\end{itemize}

These operations must satisfy the following $8$ axioms (sometimes listed as $10$ by including closure explicitly):

\subsubsection*{1. Axioms for Vector Addition}
\begin{enumerate}
    \item \textbf{Closure under addition}: For all $\mathbf{u}, \mathbf{v} \in V$, $\mathbf{u} + \mathbf{v} \in V$.
    \item \textbf{Associativity of addition}: For all $\mathbf{u}, \mathbf{v}, \mathbf{w} \in V$,
    \[
    \mathbf{u} + (\mathbf{v} + \mathbf{w}) = (\mathbf{u} + \mathbf{v}) + \mathbf{w}.
    \]
    \item \textbf{Commutativity of addition}: For all $\mathbf{u}, \mathbf{v} \in V$,
    \[
    \mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}.
    \]
    \item \textbf{Existence of a zero vector}: There exists a vector $\mathbf{0} \in V$ such that for all $\mathbf{v} \in V$,
    \[
    \mathbf{v} + \mathbf{0} = \mathbf{v}.
    \]
    \item \textbf{Existence of additive inverses}: For each $\mathbf{v} \in V$, there exists a vector $-\mathbf{v} \in V$ such that
    \[
    \mathbf{v} + (-\mathbf{v}) = \mathbf{0}.
    \]
\end{enumerate}

\subsubsection*{2. Axioms for Scalar Multiplication}
\begin{enumerate}
    \setcounter{enumi}{5}
    \item \textbf{Closure under scalar multiplication}: For all $c \in \mathbb{F}$ and $\mathbf{v} \in V$, $c \mathbf{v} \in V$.
    \item \textbf{Associativity of scalar multiplication}: For all $a, b \in \mathbb{F}$ and $\mathbf{v} \in V$,
    \[
    a (b \mathbf{v}) = (ab) \mathbf{v}.
    \]
    \item \textbf{Multiplicative identity}: For all $\mathbf{v} \in V$,
    \[
    1 \mathbf{v} = \mathbf{v},
    \]
    where $1$ is the multiplicative identity in $\mathbb{F}$.
\end{enumerate}

\subsubsection*{3. Distributive Laws}
\begin{enumerate}
    \setcounter{enumi}{8}
    \item \textbf{Distributivity of scalar multiplication over vector addition}: For all $a \in \mathbb{F}$ and $\mathbf{u}, \mathbf{v} \in V$,
    \[
    a (\mathbf{u} + \mathbf{v}) = a \mathbf{u} + a \mathbf{v}.
    \]
    \item \textbf{Distributivity of scalar multiplication over scalar addition}: For all $a, b \in \mathbb{F}$ and $\mathbf{v} \in V$,
    \[
    (a + b) \mathbf{v} = a \mathbf{v} + b \mathbf{v}.
    \]
\end{enumerate}

Then $V$ is called a \textbf{vector space} (or \textbf{linear space}) over the field $\mathbb{F}$.

That's what make the whole system works perfectly.

\section{Summary and Outlook}

As we close this chapter on linear algebra, we recognize that we have acquired more than just a collection of techniques for solving equations or manipulating matrices. We have learned a new language—the language of linearity—that reveals hidden structures throughout mathematics and science. From the elegant abstraction of vector spaces to the powerful diagonalization of transformations, linear algebra provides a universal framework for understanding relationships that are, at their heart, proportional and additive. The concepts of basis, dimension, and linear transformation form a conceptual toolkit that will serve as indispensable preparation for the deeper mathematical landscapes ahead—from the infinite-dimensional spaces of functional analysis to the curved geometries of differential manifolds. Linear algebra reminds us that simplicity and structure often underlie apparent complexity, and that the most powerful mathematics is that which provides not just answers, but clarity.

Linear algebra is fundamental not only for its elegant theoretical structure but also as a universal languagewith ubiquitous applications across science and engineering. In computer science, it underpins 3D graphics and search algorithms; in data science, techniques like PCA and SVD are core to data reduction. In physics, quantum mechanics is formulated on Hilbert spaces, and in economics, models rely on linear systems. This cross-disciplinary relevance makes linear algebra an indispensable foundation.

Theoretical development in mathematics deeply relies on linear algebraic concepts. Vector spaces generalize to modules, manifolds, and Banach spaces; linear transformations lead to operator and representation theory. Eigenvalues and eigenvectors form the basis for stability analysis in dynamical systems, network science, and quantum mechanics. Mastering linear algebra provides a key to understanding modern mathematics and theoretical science.

In advanced studies, these ideas extend into numerical linear algebra (solving large-scale systems), abstract algebra (modules over rings), and calculus (Jacobian matrices as linear approximations). From signal processing to control theory, linear algebra offers essential models and tools. Ultimately, it represents a mindsetfor uncovering linear structure within complexity, providing a powerful language for modeling, analysis, and solving problems across disciplines.


\vspace{1cm}
\noindent
\textbf{Keywords:} Eigenvalues, Eigenvectors, Diagonalization, Inner Product, Orthogonal Bases, Gram-Schmidt Process, Symmetric Matrices, Quadratic Forms \\
\textbf{References:} \\
Linear Algebra and Its Applications, 4th ed., Gilbert Strang, Cengage Learning, 2005. \\
Shanghai Jiao Tong University, School of Mathematical Sciences. Linear Algebra. China Machine Press.
